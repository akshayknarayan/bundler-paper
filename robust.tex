\subsection{Impact of Cross Traffic}\label{s:robust:cross}

\input{big_exp_nums}
\input{big_exp_text}
Can \name successfully revert to status-quo performance in the presence of buffer-filling cross traffic, then resume providing benefits once that cross traffic leaves?
In Figure~\ref{fig:eval:bigexp}, we show this scenario.
At first, the link is occupied only by \name's traffic, similar to the setup described in \S\ref{s:eval:setup}.
At time $t=60$sec, a buffer-filling cross traffic flow arrives.
\name detects its presence (indicated by the gray shading) and starts pushing more packets into the network to compete fairly, reverting back to performance that is slightly worse than Status Quo (median FCT for short flows is \bigexpElasticSlowdownWorseness higher). 
Performance is slightly worse because of the $10$ms queue that \name continues to maintain at its \inbox for active probing to detect the cross-traffic's departure, as described in \S\ref{s:queue-ctl}\footnote{We believe the benefits provided by \name in the more common regime with no competing buffer-filling cross traffic are substantial enough to make up for slight degradation in these specific scenarios.}. 
At time $t=120$sec, the buffer-filling flow stops and non-buffer-filling traffic starts, generated from the same distribution as \name as described in \S\ref{s:eval:setup}.
\name correctly detects that it is safe to resume delay-control, and continues providing scheduling benefits.
For the remainder of this subsection, we present three micro-benchmarks which dig deeper into the latter two scenarios, where cross traffic can affect \name's performance. 
We present results with both Nimbus and Copa being used as the congestion control algorithm at the \inbox. 
\cut{In \S\ref{s:eval:offeredload} we present further results on how well \name retains benefits with varying amounts of offered load.}

\input{inelastic-cross}
\paragrapha{Short-lived flows} We first consider the case where the cross traffic comprises of short-lived flows up to a few MBs.
We keep \name's offered load (comprising of web requests described in \S\ref{s:eval:setup}) constant, but vary the cross traffic's offered load.
Figure~\ref{fig:robust:cr-inelastic} shows that \name continues to provide benefits for this case, since maintaining low in-network queues is still possible in the presence of such flows.

\input{elastic-cross}
\paragrapha{Buffer-filling flows} 
%Competition from persistent backlogged flows (recall from \S\ref{s:deploy} that this condition is rare) that fill the bottleneck link's buffer is the worst-case scenario for \name.
We now evaluate how \name's throughput is impacted due to competition from varying amount of buffer-filling cross-traffic. 
%\radhika{are these all longlived flows in the bundle?}\fc{yes, 20 flows inside, 10-50 flows outside}
%As described in \S\ref{s:queue-ctl}, \name's congestion controller must push packets into the bottleneck queue to compete fairly, and thus it cannot retain packets at the \inbox for scheduling. 
%Furthermore, recall that \name maintains a $10$ms queue at the \inbox in this case. 
As discussed in \S\ref{s:queue-ctl}, in this (rare) scenario, we expect component flows in a bundle to experience slightly higher RTTs compared to \baseline.
Indeed, Figure~\ref{fig:robust:cr-elastic} shows that 
the component flows in the bundle experience  \bundlerElasticTputWorseness less throughput on average. The impact (which stems from the extra queuing at the \inbox) varies from \bundlerElasticTputWorsenessTen lower throughput with 10 competing flows to \bundlerElasticTputWorsenessFifty lower with 50. 
%\radhika{please fill -- the trend could be interesting.} 
%\an{filled}
%Note that FCTs may not increase as much; in Figure~\ref{fig:eval:bigexp}, during the elastic cross traffic period the median short flow FCT is
%\bigexpElasticSlowdownWorseness higher.
%\radhika{edited this. basically wanted to bring up the `performance is worse point' sooner when we discuss Fig 8, as opposed to towards the end of section, as it would have confused readers.}

\input{twobundler}
\paragrapha{Competing Bundles} Last, we evaluate the case where flows from multiple bundles compete with one another. 
In Figure~\ref{fig:robust:twobundler}, we show the performance with two bundles of traffic competing with one another at the same bottleneck link. 
Both bundles comprise of web requests along with a backlogged Cubic flow. 
%Even in the presence of this buffer-filling cross traffic, 
%\radhika{??? i believe the backlog flow is in the bundles and not outside it}
%\an{yes, it is, that's why the perf is good. I agree we should reword, it is confusing}
Both bundles maintain low queueing in the network and successfully control the queues at the \inbox.
Thus, \name provides benefits for both bundles, even when the amount of traffic in each bundle is different.  

\subsection{Impact of Congestion Control}\label{s:eval:cc}

%\an{I have cut down this subsection: removed the e2e congestion control figure and replaced with inline text}
We now evaluate the impact of a different congestion control algorithm running at the \inbox and at the endhosts.

\input{congestion-control}

\Para{\capinbox congestion control} So far we have evaluated \name by running Copa~\cite{copa} at the \inbox. 
Figure~\ref{fig:eval:cc} shows \name's performance with other congestion control algorithms (namely, Nimbus's BasicDelay~\cite{nimbus-arxiv} and BBR~\cite{bbr}), and using SFQ scheduling. 
We find that using BasicDelay provides similar benefits over \baseline as Copa. 
BBR, on the other hand, performs slightly worse than \baseline. 
This is because it pushes packets into the network more aggressively than the other schemes, resulting in a bigger in-network queue.
This, combined with the queue built at the \name, results in the endhosts experiencing higher queueing delays than \baseline. This shows that the choice of congestion control algorithm, and its ability to maintain small queues in the network, plays an important role. 

\Para{Endhost congestion control} We used Cubic congestion control at the endhosts for our experiments so far. When we configure endhosts to use Reno or BBR, \name's benefits remain: \name achieves 58\% lower FCTs in the median compared to the updated \baseline where the endhosts use BBR. 
This shows that \name is compatible with multiple endhost congestion control algorithms.
%This is primarily because in the \baseline, using BBR causes endhosts to achieve 66\% worse median slowdown ($1.62$ with Cubic to $2.68$ with BBR); \name's slowdown is only 5\% worse when endhosts use BBR ($1.08$ with Cubic to $1.14$ with BBR). 

\cut{
\input{e2e}
\Para{Endhost congestion control} 
We used Cubic congestion control at the endhosts for our experiments so far. When we configure endhosts to use Reno or BBR (as implemented in Linux $4.13$), \name's benefits remain (Figure~\ref{fig:eval:traffic}).
%: in Figure~\ref{fig:eval:traffic}, \name achieves 58\% lower FCTs in the median compared to the updated \baseline where the endhosts use BBR. 
%This is primarily because in the \baseline using BBR causes endhosts to achieve 66\% worse median slowdown ($1.62$ with Cubic to $2.68$ with BBR); \name's slowdown is only 5\% worse when endhosts use BBR ($1.08$ with Cubic to $1.14$ with BBR). 
This shows that \name is compatible with multiple endhost congestion control algorithms.
}
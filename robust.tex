\subsection{Impact of Cross Traffic}
\label{s:robust:cross}

\an{put big experiment here}

In studying the impact of cross traffic, we present results with both Nimbus and Copa being used as the congestion control algorithm at the \inbox.

\input{inelastic-cross}
\paragrapha{Short-lived flows} We first consider the case where the cross traffic comprises of short-lived flows up to a few MBs.
We keep \name's offered load (comprising of web requests described in \S\ref{s:eval:setup}) constant, but vary the cross traffic's offered load.
Figure~\ref{fig:robust:cr-inelastic} shows that \name continues to provide benefits for this case, since maintaining low in-network queues is still possible in the presence of such flows.

\input{elastic-cross}
\paragrapha{Buffer-filling flows} Competition from persistent backlogged flows that fill the bottleneck link's buffer is the worst-case scenario for \name (as discussed in \S\ref{s:deploy}).
As described in \S\ref{s:queue-ctl}, \name's congestion controller must push packets into the bottleneck queue to compete fairly, and thus it cannot retain packets at the \inbox for scheduling. 
Furthermore, since \name maintains a $10$ms queue at the \inbox in these cases, we expect a small amount of RTT unfairness. \an{maybe don't say this?}
We indeed see this in Figure~\ref{fig:robust:cr-elastic}, as we vary the number of competing Cubic cross-traffic flows.

\input{twobundler}
\paragrapha{Competing Bundles} Last, we evaluate the case where flows from multiple bundles compete with one another. 
In Figure~\ref{fig:robust:twobundler}, we show the performance with two bundles of traffic competing with one another at the same bottleneck link. 
Both bundles comprise of web requests along with a backlogged Cubic flow. 
Even in the presence of this buffer-filling cross traffic, both bundles maintain low queueing in the network and successfully control the queues at the \inbox.
Thus, \name provides benefits for both bundles, even when the amount of traffic in each bundle is different.  

\subsection{Impact of Congestion Control} \label{s:eval:cc}

We now evaluate the impact of a different congestion control algorithm running at the \inbox and at the endhosts.

\input{congestion-control}

\Para{\capinbox congestion control} So far we have evaluated \name by running Copa~\cite{copa} at the \inbox.  
Figure~\ref{fig:eval:cc} shows \name's performance with other congestion control algorithms (namely, Nimbus~\cite{nimbus} and BBR~\cite{bbr}), and using SFQ scheduling. 
We find that using Nimbus provides similar benefits over \baseline as Copa. 
BBR, on the other hand, performs slightly worse than \baseline. 
This is because it pushes packets into the network more aggressively than the other schemes, resulting in a bigger in-network queue.
This, combined with the queue built at the \name, results in the endhosts experiencing higher queueing delays than \baseline. This shows that the choice of congestion control algorithm, and its ability to maintain small queues in the network, plays an important role. 

\input{e2e}
\Para{Endhost congestion control} 
We used Cubic congestion control at the endhosts for our experiments so far. When we configure endhosts to use BBR (as implemented in Linux $4.13$), \name's benefits remain: in Figure~\ref{fig:eval:e2e}, \name achieves 58\% lower FCTs in the median compared to the updated \baseline where the endhosts use BBR. 
This is primarily because in the \baseline using BBR causes endhosts to achieve 66\% worse median slowdown ($1.62$ with Cubic to $2.68$ with BBR); \name's slowdown is only 5\% worse when endhosts use BBR ($1.08$ with Cubic to $1.14$ with BBR). 
This shows that \name is compatible with multiple endhost congestion control algorithms.
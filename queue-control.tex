\section{Coping with Cross Traffic}\label{s:queue-ctl}

Recall that \name's key trick is to use delay-minimizing congestion control algorithms from the bottleneck to the \inbox.
However, a well-known property of delay-minimizing congestion control algorithms is that when competing with traditional window-probing controllers, they lose throughput~\cite{copa}.
Therefore, it is important for congestion controllers at the \inbox to detect the presence of such algorithms and disable the use of delay-control in these situations.
With delay-control disabled, the \emph{end-to-end} congestion control algorithms controlling \name's component traffic will fill buffers at the true bottleneck instead of at the \inbox.
Because the \inbox will no longer have queueing, component traffic will fall back to status-quo performance.
Buffer-filling cross traffic is rare~\cite{something}, so once such cross traffic goes away, \name's congestion controller will resume building queues and applying scheduling policy.

This leaves us with two questions: 
(1) what mechanisms do we need to support the cross-traffic detection module (\S\ref{s:qctl:nimbus})? and
(2) once \name detects buffer-filling cross traffic, how should it react (\S\ref{s:qctl:ctl})? 

\subsection{Active Probing}\label{s:qctl:nimbus}
Because bundles include buffer-filling cross traffic, it is important to distinguish between \emph{self-inflicted} queueing delay and queueing delay due to cross traffic.
When the queueing delay is purely self-inflicted, it is safe to resume control over the queues at the \inbox.
Passive probing is insufficient to determine this state, since passive measurements of the bottleneck will be identical in the case of self-inflicted queueing and queueing driven by cross traffic.
Therefore, it is important to \emph{actively probe}, that is, change the rate of in-band traffic and observe the response of the cross traffic. 
This is what the Nimbus mechanism does.
At a high level\footnote{A full description and evaluation of Nimbus is in concurrently submitted work, Paper \an{XX}}, 
Nimbus sinusoidally varies the sending rate $r(t) = A sin(\frac{4\pi{}t}{T})$ during the up-pulse, where $A$ is the pulse amplitude (set to one-fourth of the estimated bottleneck bandwidth) and $T$ is the pulse duration, and measures the cross traffic's response in the frequency domain.
The \inbox can use the Nimbus algorithm to detect when to relinquish control over the queue by interposing this sending pattern over the delay-controller's rate decisions.

\subsection{Fairness Control}\label{s:qctl:ctl}
What should the \inbox do when Nimbus indicates the presence of buffer-filling cross traffic?
A naive approach would be to emulate the end-host behaviour and run Cubic. This approach has two shortcomings.
First, it must solve the MulTCP~\cite{multcp} problem: a bundle may comprise of multiple Cubic connections, and to achieve a fair bandwidth share it should therefore compete as aggressively as the number of component flows. On some high-performance datapaths, it may be difficult to measure this number~\cite{heavy-hitters}.

We propose a simpler solution: since bundles already comprise of connections that end-hosts control, if we simply \emph{let the traffic pass} the bundle will naturally compete fairly with cross traffic, just as traffic in the Internet does today. 
However, we must be careful when letting the traffic pass; if the \inbox entirely drains its queues into the network, it will no longer be possible for Nimbus to overlay pulses onto the traffic pattern, and it will be unable to determine the nature of the cross traffic.
Practically, this would mean that once \inbox switches to compete with cross traffic, it would never gain the information necessary to switch back.

Therefore, to support active probing while also letting the traffic pass, the \inbox should hold back some packets while letting the remainder pass.
How many packets should this be? The \inbox should be able to generate enough packets for a Nimbus up-pulse, \ie the area under the up-pulse curve: 
$A \int_0^{\frac{T}{4}} sin(\frac{4\pi{}t}{T}) dt = \frac{AT}{2\pi}$.
From Nimbus, we use $T = 0.2$ seconds and $A$ is as above, one-fourth the bottleneck bandwidth. By Little's law, we can calculate the amount of extra queueing: $\frac{T}{8\pi}$, or $8$ms.
We thus configure the \inbox to hold back $8$ms of queueing for active probing.
Note that this extra queueing is in addition to queueing in the network. As a result, the end-to-end Cubic connections will see RTT inflation, and experience slightly lower throughput due to RTT unfairness. In \an{forward pointer to eval} we show that \name still achieves status quo performance.

To control the queue to $8$ms, we implement a PI controller at the \inbox as part of the fairness control module. 
It overlays a rate $r(t) = \alpha (q - q_T) + \beta (\dot{q})$, where $q$ is the queue size and $q_T$ is the target queue size computed above.
\an{We experimentally determine $\alpha = 100$ and $\beta = 10000$.}

\cut{
\begin{outline}
\1 Nimbus relies on pulsing the sending rate, and analyzing the resulting response to this pulse in received rate and rtt in the frequency domain. \radhika{Maybe add a line about Copa too and say why its not as suitable}
    \2 This poses two problems which are not present in the traditional end-host congestion control setting. \radhika{The transition from previous statement is not too clear. Might want to say something like ``This requires control over the input / data being sent (explain what you mean by control and why Nimbus needs it). Such control is naturally present when the detector runs at the endhosts (as explored in Nimbus) and when Bundler's \inbox is in delay control mode (i.e. no elastic flow has been detected), but it is lost once Bundler gets out of the delay-control mode upon detecting elastic cross traffic to relinquish its queue control and fall back to status quo...(what you have in the next bullet point). So how do we detect when the elastic cross traffic has left, so that we can switch back to delay mode?''}
        \3 Lack of control over input
        \3 Dynamic number of underlying connections \radhika{Why is this an issue? As in, why do you need to know the number of connections?}
    \2 Both these problems confound the usual approach of falling back to cubic when buffer-filling cross traffic is detected.
        \3 Lack of control over input: running Cubic at \inbox will drain the queues into the network. As a result, it will no longer be possible to overlay pulses onto the traffic pattern, and Nimbus will be unable to determine the nature of the cross traffic.
            \4 
        \3 Dynamic number of underlying connections: A naive approach of running a single instance of cubic would cause unfairness to bundled traffic
            \4 Instead of competing with the aggressiveness of $n$ cubic flows, it would only compete as one.
            \4 Determining the number of currently-backlogged component connections can be difficult on some datapaths~\cite{heavy-hitters} \radhika{Ok, now I see the issue with dynamic connectins. But this is a second orthogonal issue to the first one. Maybe explain one problem at a time. Also, are we really running Cubic in the `get out of the way mode'? This part needs a bit more build up. If we are not running Cubic, we can directly jump to queue control, and we don't have to look into this. Unless, we say something like: `` But this has the issue that....''.}
\1 Queue-control to support nimbus pulsing
    \2 We implement a congestion control scheme which seeks to keep a small, constant number of packets at the \inbox while letting other traffic pass through.
        \3 This does not interfere with the nimbus pulse analysis because it appears as high-frequency noise in the frequency domain. \radhika{not sure I understand this point}
        \3 Because traffic is passing through, end-to-end congestion controllers will naturally build queues at the botteneck and compete fairly with the cross traffic.
        \3 Because \inbox maintains a small queue, component connections will experience a small degree of rtt-unfairness.
            \4 Circumvent by using an end-to-end scheme which doesn't suffer from rtt-unfairness?
        \3 This unfairness is somewhat offset by the scheduling within the small queue maintained at the \inbox. \radhika{hopefully we will have results showing this, that we can add a forward pointer to?}
\1 Computing \inbox queue target
    \2 We can look at the shape of the nimbus pulse to determine what the queue target should be: $A \mu \int_0^{\frac{T}{4}} sin(4 \pi \frac{t}{T}) dt$.
        \3 $A$ is pulse size fraction
        \3 $\mu$ is bottleneck bandwith estimate
        \3 $T$ is pulse duration, depends on pulse frequency.
\1 We implement a simple PI controller to maintain this queue at the \inbox.
    \2 $rate = \alpha (q - q_T) + \beta (\dot{q})$
        \3 We use $\alpha = 100$ and $\beta = 10000$.
    \2 \fc{todo: explain principle of $\alpha$ and $\beta$}
\end{outline}
}

% add plot varying queue length
% maybe measure the amount of time nimbus spent in the correct mode

% 3 iterations of the big experiment for comparison: reasonable qlen, too low, and too high

% 12: constant 48Mbps in the bundle and vary the cross traffic, don't vary within bundle, we expect it won't vary but we'll see
% 15: vary the amount of traffic in the bundle, larger range
% get both and then decide what to show 

% 14: broader number of bundles

\section{Implementation}\label{s:impl}
\begin{figure*}[t]
    \centering
    \includegraphics[width=2\columnwidth]{img/bundler-diagram}
    \caption{\name Implementation Overview.}\label{fig:bundler}
\end{figure*}

\cut{The prevalence of middleboxes today means that there are multiple options for implementing a \name \radhika{cut this line?}.}


\Para{\inbox} Our \inbox implementation comprises of a \emph{data plane} and a \emph{control plane}. The data plane is responsible for (i) packet forwarding, (ii) tracking the number of bytes sent in the last epoch, (iii) reporting epoch boundary packets and the feedback from the \outbox to the control plane,  (iv) enforcing a sending rate (computed by the control plane) on a bundle, and (iv) enforcing the desired scheduling policies on the traffic within a bundle. The control plane is responsible for (i) measuring congestion signals and computing epoch sizes using the information provided by the data plane, and (ii) running the congestion control algorithm for each bundle to compute appropriate sending rates based on the congestion signals. 

\Para{\outbox} The \outbox is simple and does not require a control plane. It simply tracks the number of bytes and sends feedback messages to the \inbox on observing epoch packets.

% We describe a two-part design for the \inbox: a ``control plane'' and ``data plane'' separation.
% The data plane is responsible for packet forwarding, maintaining a count of the \cut{in-bundle} bytes sent, enforcing a rate and scheduling policy on the bundle, and reporting epoch boundary packets to the control plane.
% These operations can be implemented either in software, via modern platforms for network function virtualization~\cite{bess, click, mos, netbricks}, or in hardware via programmable switches~\cite{p4} \radhika{move to end, see comment at end of paragraph}.
% The control plane is responsible for computing measurements by combining feedback from the data plane and \outbox, and running the congestion control logic necessary to pick the appropriate rate for each bundle.
% The \outbox is simple and does not require a control plane; it simply must observe the packet stream, maintain a byte count, and send reports to the \inbox on observing epoch packets.
% The \outbox can, therefore, be implemented entirely in hardware if necessary \radhika{cut}.
% \radhika{We envision the control plane to be implemented in software, while the data plane can be implemented in...(the line you had before)}

\subsection{\name Event Loop}\label{s:impl:loop}
Figure~\ref{fig:bundler} overviews the operation of our \name implementation.

(1) Packets arriving at the \inbox are sent to the qdisc. Those that match a bundle are put into the proper queue, 
otherwise they are forwarded immediately. (2) The qdisc observes a packet that matches the epoch boundary
condition (\S\ref{s:measure:marking}). (3) It sends a netlink message to the \inbox process running in user-space, which records the packet
in its epoch data structure, then forwards the packet along. (4) The \outbox observes the same epoch boundary
packet. (5) It sends an out-of-band UDP message to the \inbox containing the hash of the packet and its current state. 
(6) The \inbox receives the UDP message, and uses it to calculate the epochs and measurements as described 
in \S\ref{s:measurement}.

(7) Asynchronously, the \inbox invokes the congestion control algorithm every $10$ms\footnote{Prior work shows~\cite{ccp} that the datapath need only invoke congestion control algorithms only on RTT-timescales, and evaluates congestion control algorithms on their sensitivity to this period.}.
via \texttt{libccp},
giving it a chance to observe any new measurements and change its behavior. (8) If it decides to update
the bundle rate or congestion window, the \inbox communicates this to the qdisc
using \texttt{libnl}. Since the qdisc only supports rate enforcement, if the algorithm
also uses a congestion window, the \inbox computes $\text{\emph{effective rate}} = \frac{\text{CWND}}{\text{RTT}}$
and sets the rate of the qdisc to the minimum of the explicit rate and this effective rate.
Upon receiving new RTT samples, the \inbox updates the effective rate, and updates the qdisc rate if appropriate.
Finally (9), if the \inbox changes the desired epoch length based on new measurements, it communicates this to the \outbox, also out-of-band.

%\radhika{i liked this subsection!}

\subsection{Prototype}\label{s:impl:prototype}
For our prototype, we implement the \inbox data-plane using Linux \texttt{tc}~\cite{tc}, and we limit our implementations of scheduling policy to those already widely available in the Linux kernel.
We patch the TBF queueing discipline (qdisc)~\cite{tbf} to detect epoch boundary packets, transmit feedback to the control plane using a netlink socket, and make the internal scheduling policy configurable.
We express scheduling policy by changing TBF's ``inner\_qdisc'' from the default FIFO to a work-conserving, scheduling qdisc: we support SFQ~\cite{sfq} and FQ-CoDel~\cite{fq-codel}.
Our patch to the TBF qdisc comprises $112$ lines of C.
\an{cut?}\an{Additionally, we remove one line in the rate update to disable re-filling the token bucket on rate changes to prevent a penalty for frequent rate updates\footnote{by default, TBF instantaneously re-fills the token bucket when the set rate changes. In our implementation, the \inbox sets the rate frequently, causing rate fluctuations. We therefore disable this functionality.}.}

We implement the \inbox control plane to run in user-space in $1167$ lines of Rust. 
It uses \texttt{libnl} to communicate with the qdisc, and \texttt{libccp}~\cite{ccp} to communicate with the congestion control algorithm via Unix-domain sockets.
It maintains the congestion control state of each bundle, but does not maintain (or observe) per-flow state.
%\radhika{no. of LoC for \inbox dataplane?} \an{first paragraph of this section?}

We implement the \outbox using \texttt{libpcap} in $188$ lines of Rust. It listens for packets on the interface, checks for epoch packets, and sends the reports to the \inbox. It must maintain per-bundle counters (described in \S\ref{s:impl:discovery}).

\radhika{We use the Fowler/Noll/Vo (FNV) hash function~\cite{fnv-hash}, a non-cryptographic fast hash function with a low collision rate.}

\radhika{an experiment showing scalability of bundler implementation?}

\subsection{Congestion Control}\label{s:impl:cc}
We use existing implementations of congestion control algorithms on CCP~\cite{ccp}, a platform for expressing congestion control algorithms in an asynchronous format; since CCP algorithms are already designed to receive network feedback asynchronously, they are a natural choice for our epoch-based measurement architecture. \radhika{this sentence should comer earlier, maybe when you first mention ``control-plane'' at the beginning of Sec 5, especially since 5.2 mentions CCP and libccp.}

\an{move to earlier, maybe design:}
It is important to note that traditional loss-based congestion controllers (\ie Cubic~\cite{cubic}, NewReno~\cite{newreno}) are poor choices for \name. 
This is because they double-penalize component traffic for losses: first, in the reaction of the end-to-end congestion controller to the loss, and second in the reaction of the controller at \name.

\an{missing connection here}
%Therefore, for algorithms which observe loss (Nimbus and Copa), we configure them to ignore it; the end-to-end reaction to loss will modulate the offered load to be the fair share of the bottleneck.
%
Further, we make a useful distinction between two types of cross-traffic conditions \name will experience.
We consider \emph{elastic} cross traffic and \emph{inelastic} cross traffic.
Elastic cross traffic (\eg Cubic, NewReno) probes for bandwidth and fills buffers at the bottleneck; inelastic cross traffic (\eg short flows) has limited demand and does not fill buffers.

When cross traffic is inelastic, the queueing delays that \name encounters will be self-inflicted. In these scenarios, scaling back and adjusting the sending rate at \inbox will cause the bottleneck to shift and the queueing delays at the bottleneck to reduce.
In the presence of elastic cross traffic, however, congestion control algorithms \emph{must} push packets into the bottleneck queue to remain competitive~\cite{bbr, copa, nimbus}.
As a result, we expect \name's benefits to disappear, since it must relinquish control of the queues to the bottleneck link.

\radhika{if above parts are covered in Sec 3, add a backwards pointer, and start with the implementation details below.}

Recent work~\cite{copa, nimbus} has considered the problem of switching between modes to take advantage of low queueing delays when it is possible to do so, yet reacting to and competing fairly with elastic cross traffic when it present.
Unfortunately, it is not immediately clear how to determine \name's fair rate once we detect elastic cross traffic.
The number of individual component connections which are backlogged at any given time may vary, and determining which connections should count towards \name's aggregate fair share rate would thus require per-connection state \radhika{not super clear to me}.

We sidestep this issue with a simple, yet powerful, observation: rather than attempt to compete with elastic cross traffic, \name should \emph{get out of the way} of its component connections.
The end-to-end congestion controllers responsible for \name's component connections will then naturally compete with and achieve their fair rate against elastic cross traffic.

It is important to implement this approach carefully. 
Consider a naive implementation in which upon detecting elastic cross traffic, \name stopped pacing packets entirely.
This approach would be unable to detect when the elastic cross traffic subsided, since it yields all control over the network queues.

Thus, when \name's congestion control algorithm detects that the cross traffic is elastic, it uses a simple control rule: \texttt{rate <- observed\_receive\_rate * 1.25}. 
This control rule will rapidly grow the pacing rate at the \inbox until it stabilizes at 25\% above the offered load.
Then, using the elasticity detection mechanism described in prior work~\cite{nimbus}, \inbox can modulate the pacing rate to determine when to retake control of the queueing, and therefore the scheduling policy.

\an{move the elastic cross traffic comparison results here?} \radhika{no, add a forward pointer}
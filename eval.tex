\section{Evaluation}\label{s:eval}

We evaluate the following aspects of our design:
\begin{enumerate}
    \item To what extent can scheduling policies in \name improve performance?
    \item Is \name able to shift the bottleneck enough to enforce scheduling policies? We discuss this in \S\ref{s:robust}.
\end{enumerate}

\subsection{Flow Completion Time}\label{s:eval:fct}

\input{overview-benefits}

We evaluate our implementation of \name (discussed in \S\ref{s:impl}) using network emulation via mahimahi~\cite{mahimahi}.
There are three machines in our setup: one machine is a traffic generator, another is configured as a middlebox, and a third is the receiver.
We run an empirical traffic generator, in which a many-threaded client generates requests from a given empirical request size CDF and sends them to one of $200$ server processes on the traffic generator.
Each server then responds to the client with the requested amount of data.
We then measure the flow completion time of each client request.
Note that it is important to use many server processes so that the effect of head-of-line blocking on the client requests is limited.
Indeed, all the CDFs presented in this section have a ``knee'' where the effects of head-of-line blocking become apparent.

We first run \name with no cross traffic.
This scenario highlights the benefits to be gained by managing any \emph{self-inflicted} overhead in the FCT caused by FIFO scheduling. 
We explore the effects of externally inflicted bottleneck link conflicts in \S\ref{s:robust}.

On a $96$Mbps link, we generate $84$Mbps of offered request load from a simple CDF with a heavy-tailed mix of short and long requests: 90\% of the requests are less than $5$KB, and 100\% of the requests are less than $1$MB. 
Each CDF shown in this section is comprised of $100,000$ requests sampled from this distribution, across $10$ runs each with a different random seed.
In addition, there is a persistently backlogged connection inside the traffic aggregate.
We use a Copa~\cite{copa} congestion controller at the \inbox to manage the bottleneck queue, and the stochastic fair queueing~\cite{sfq} scheduling policy. 
The results are in Figure~\ref{fig:eval:best}.
With \name, the median flow completion time (FCT) decreases from $147$ms using FIFO scheduling at the bottleneck link (the baseline configuration) to $58$ms with \name: 61\% lower.

Where do these benefits come from? We devote the remainder of \S\ref{s:eval:fct} to exploring this question.

\paragrapha{Necessity of scheduling} It is important to note that \name is not necessarily a means of achieving low end-to-end delays, as promised by recent efforts in congestion control~\cite{copa, nimbus}, despite its use of modern congestion control algorithms. 
To see why this is the case, recall that \name does not modify the end-hosts: they continue to run the default Cubic congestion controller, which will probe for bandwidth until it observes loss.
Indeed, the packets end-host Cubic sends beyond those the link can transmit must queue somewhere in the network or be lost. Without \name, they queue at the bottleneck link.
With \name, they instead queue at the \inbox.
With adequate scheduling, this extra induced queueing will affect packets from the long flows rather than short flows.

\input{fifo-not-enough}

We can see this by measuring the relative performance of using FIFO scheduling at the \name compared to the SFQ scheduling in Figure~\ref{fig:eval:best}.
The results are in Figure~\ref{fig:eval:fifo}. 
Unsurprisingly, the FCTs with FIFO scheduling at \name are \emph{worse}: with a median FCT of $173$ms, it is $18$\% higher than the baseline.

\paragrapha{Impact of congestion control} How does our choice of congestion control impact our results? 
It is important to choose a congestion control algorithm carefully based on the known or inferred characteristics of the bottleneck link. 
In Figure~\ref{fig:eval:cc} we show three congestion control protocols: BBR~\cite{bbr}, Nimbus~\cite{nimbus}, and Copa~\cite{copa}.
In this scenario, it is important to control delays. Nimbus and BBR both maintain slightly higher queueing delays at the bottleneck link, and thus they achieve higher median FCTs \an{numbers}.

\input{congestion-control}

\paragrapha{Offered load} Naturally, if a link is less congested, scheduling the packets that traverse it will have less benefit. Accordingly, as we reduce the offered load in this experiment, we expect the gains from scheduling to diminish. The result is in Figure~\ref{fig:eval:offeredload}. We reduce the offered load by removing persistently backlogged connections from the workload and generating individual requests at 50\%, 75\% and 87.5\% of the bottleneck link bandwidth. 
At 87.5\% load, even without the load offered by a persistenly backlogged connection, \name improves FCTs by \an{amount}. 
As the offered load decreases to 50\%, the benefit provided by \name decreases as well -- to \an{amount} at the 75th percentile.

\input{varying-offered-load}

\paragrapha{Terminating TCP Connections} An alternate implementation choice for a \name would use a TCP proxy to terminate connections at the \inbox.
We consider the best-case outcome of terminating TCP connections --- the effective RTT observed by the end-to-end congestion controller would decrease (since the proxy would acknowledge its segments much faster than the original receiver would), and it would grow its window rapidly.
A proxy-based design would then apply scheduling policy to component traffic (recall that we use SFQ).
We emulate this best-case scenario by increasing the end-hosts' initial congestion window from the default $10$ to $1000$ segments.
Now, even the largest $1$MB ($\approx700$ packet) flows in this scenario can immediately transmit all their segments to the \inbox.
The result is in Figure~\ref{fig:eval:proxy}.

\input{with-proxy}

Surprisingly, terminating TCP connections does not help the short flows (longer flows indeed see an improvement). 
To see why, we refer back to Figure~\ref{fig:eval:fifo} --- absent scheduling, \name can cause performance degradation because it induces higher overall queueing delays.
In another instantiation of the same effect, rapid window growth from all active connections causes rapid queue growth at the \inbox. The resulting increased effect end-to-end delays cancel out the gains of scheduling.

\subsection{Rate Stability}\label{s:eval:ratestable}

\begin{outline}
    \1 \name can improve the rate stability of component traffic
    \1 One such class of traffic is video streaming
\end{outline}

\cut{
\begin{outline}[enumerate]
    \1 Bundler can reduce flow completion times
        \2 Base case with no cross traffic: run empirical traffic generator in the bundle varying offered load from 30\% to 90\%
            \3 Rough estimate of current results: 2-3x better for short flows (normalized fct, and only 10-20\% worse for long flows
            \3 Show how close bundler is to ideal situation (bottleneck router doing FQ scheduling) without actually having to change the middle of the network
        \2 Add inelastic background traffic
        \2 Add elastic background traffic
            \3 Have to enable x-tcp mode (perhaps dynamic number of flows) for bundle
            \3 can measure number of flows by looking at the number of backlogged queues
    \1 Microbenchmarks
        \2 Show that the bundler architecture is capable of emulating rate-based congestion control algorithms: \textbf{PLOT} throughput and delay of Nimbus/BBR/etc with and without bundler show that they look the same and achieve the same average behavior 
            \3 Regardless of underlying number of flows
        \2 Show that the measurements obtained via packet sampling are robust even though epoch size is varying over time due to hashing
        \2 Show why dynamic epoch adjustment is needed 
            \3 If too small relative to rate, there's too much overhead, if too large relative to rate the algorithm can't make progress
            \3 \textbf{PLOT} show BBR with large sampling rate, dies after going into probe RTT because it doesn't get any more measurements 
            \3 Show effect of window of measurements
                \4 important: there are no real ``magic parameters'' here: \name should take measurements in 1-rtt windows, and epochs should be as short as performance considerations allow.
        \2 Standard fairness experiment: 3 background cubic flows not in the bundle, vary the number of flows in the bundle, show that the bundle is able to achieve the "fair" share of the link 
    \1 Bundler allows long running flows to experience better rate stability, the rate of the bundle may fluctuate over time, but long running flows in the bundle see a stable rate while shorter flows 
        \2 Application: could show how this benefits e.g. live video 
    \1 Multiple bundles competing with each other?
    

\end{outline}
}

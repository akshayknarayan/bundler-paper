\section{Evaluation}\label{s:eval}

We have seen in Figure~\ref{fig:design:shift-bottleneck:after} that \name can move the network queues to the \inbox to gain control over scheduling. 
We now evaluate the following aspects of our design.
\begin{enumerate}
    \item Where do \name's performance benefits come from? We discuss this in the context of improving the flow completion times of \name's component flows. (\S\ref{s:eval:fct})
    \item Can \name enforce different scheduling policies? (\S\ref{s:eval:policies})
    \item Under what conditions do \name's performance benefits hold? (\S\ref{s:robust:cross})
\end{enumerate}

\subsection{Understanding Benefits}\label{s:eval:fct}

\paragrapha{Experimental Setup}
We evaluate our implementation of \name (discussed in \S\ref{s:impl}) using network emulation via mahimahi~\cite{mahimahi}.
Mahimahi creates a Linux network namespace and applies some network condition (emulated delay or link bandwidth) to all packets that traverse it.

There are three $8$-core machines in our setup: one machine is a sender, another is configured as a middlebox and runs an \inbox, and a third is the receiver. Throughout our experiments, CPU utilization on the machines remained below $10$\%.
It is important for experiment fidelity to run the sending application on a different machine than the \inbox; otherwise, because the \inbox data-path uses tc (\S\ref{s:impl}), TCP small queues (TSQ) unrealistically avoids queueing.
The \outbox runs on the same machine as the receiver, inside the same mahimahi network namespace as the receiving application.
\an{somebody make sure this makes sense}\an{Since our prototype implementation uses the Linux kernel datapath, we disable TCP segmentation offload (TSO) and generic receive offload (GRO) on the machines to maintain a consistent view of packet headers between the \inbox and the \outbox.
This is an artifact of our prototype implementation of the \outbox using \texttt{libpcap}: in a real deployment, the \outbox would have the same view of the packets as the \inbox.}

The application we use emulates web traffic: a many-threaded client generates requests from a given empirical request size CDF~\cite{caida-dataset} and assigns them to one of $200$ server processes on the traffic generator.
Each server then sends the requested amount of data to the client and
we measure the flow completion time of each such request.
We use multiple server processes to emulate multiple machines behind the \name at the customer's edge.

Each CDF shown in this section is comprised of $100,000$ requests sampled from this distribution, across $10$ runs each with a different random seed.
In addition, there is a persistently backlogged connection inside the traffic aggregate that models a background large data transfer which would normally fill the bottleneck queue.
%
\cut{
\radhika{Another way to structure the the rest of this section: /*}
\begin{itemize}
    \item Subsection: ``Understanding \name's performance benefits''. 
    \begin{itemize}
        \item (This will cover most of what you have under ``Flow Completion Time'')
        \item Experiment setup (what you currently have)
        \item Result: (Figure~\ref{fig:eval:best})
        \item Explaining why
        \begin{itemize}
            \item Effective bottleneck shifting: a graph that shows queuing delay at mahimahi without bundler, and graphs that show the queuing delay at mahimahi and inbox when using bundler. \an{(this is figure~\ref{fig:design:shift-bottleneck}, should we repeat it?)}
            \item Scheduling is necessary: comparison with FIFO -- what you have now
        \end{itemize}
        \item Comparison with ideal case: mahimahi link does sfq. Show we are not too bad (is it possible to get this result?). \an{will try, but it may not be possible to run sfq in mahimahi}
    \end{itemize}
    \item Subsection: ``Using \name to enforce different policies''.
    
    \begin{itemize}
        \item We use same set-up as before, but different scheduling and queue management polcies at the \name to meet different requirements. 
        \item Improved FCTs using FQ: Helps short flows achieve a smaller FCT by reducing the queuing delay that they see. Results already shown in previous subsection. 
        \item Smaller Queuing Delay using AQM: Helps keep queuing delay small. Present another FCT graph like Figure~\ref{fig:eval:best}, but with FQ-CoDel or PI. Also present a graph which shows the CDF of per-packet delays (if possible). Also, if possible (and if space), a comparison with properly configured and badly configured AQM scheme in the mahimahi link.
        \item Improved Rate Stability 
        \item Prioritizing interactive traffic
        \item anything else? 
    \end{itemize}
    \item Subsection: Robustness analysis
    \begin{itemize}
        \item We vary different aspects of the experiment setup detailed earlier to see how they impact our key results. We use SFQ as the scheduling policy in the bundler, but similar trends would hold for other policies (?). 
        \item Impact of congestion control (what you have now) [might also be fine if it goes in the previous subsection]
        \item Impact of offered load (what you have now)
        \item Impact of path characteristics
        \item Alternative design choices (TCP Proxy)
        \item Impact of cross traffic
        \begin{itemize}
            \item Cross traffic in it's own bundle
            \item Cross traffic not using bundle
        \end{itemize}
    \end{itemize}
\end{itemize}
\radhika{*/}
}
%
%\radhika{another title for this section, or some preceding explanation on what each section is about}
%
%\cut{Note that it is important to use many server processes so that the effect of head-of-line blocking on the client requests is limited.
%Indeed, all the CDFs presented in this section have a ``knee'' where the effects of head-of-line blocking become apparent.
%}
%\radhika{when the graphs are presented, explain the `knee' as an artifact of our expts caused due to some HoL blocking. Btw, are you sure that's why you have the knee?}

\input{overview-benefits}
\an{"Baseline" -> "Status Quo"}
\an{"Optimal" -> "In-Network"}
\paragrapha{Comparison with Optimal and Baseline} 
We first present results for a simplified scenario without any cross-traffic, i.e. all traffic traversing through the network is generated by the same customer and is, therefore, part of the same bundle. 
This scenario highlights the benefits of using \name when the congestion on the bottleneck link in the network is self-inflicted by a single customer. We explore the effects of queues at the bottleneck caused by other cross-traffic in \S\ref{s:robust:cross}.

On a $96$Mbps link, we generate $84$Mbps of offered request load from a CDF with a heavy-tailed request size CDF drawn from an Internet core router~\cite{caida-dataset}.
The workload is heavy-tailed: 97.6\% of requests are 10KB or shorter, and the largest 0.002\% of requests are between $5$MB and $100$MB.

We evaluate three configurations. 
The ``Baseline'' configuration represents the status quo: the \inbox simply forwards packets as it receives them, and the mahimahi bottleneck uses FIFO scheduling.
The ``Optimal'' configuration deploys fair queueing\footnote{
We implement this scheme by modifying mahimahi (our patch comprises $171$ lines of C++) to add a packet-level fair-queueing scheduler to the bottleneck link.}
at the mahimahi bottleneck. 
Recall from \S\ref{s:intro} that this configuration is not deployable in practice: it would force all customers --- who may desire diverging scheduling policies --- to use the same scheduler.
Finally, for the \name configuration, we use a Copa~\cite{copa} congestion controller at the \inbox to manage the bottleneck queue, and the stochastic fair queueing~\cite{sfq} scheduling policy. 

Figure~\ref{fig:eval:best} presents the results with the above experiment setup.
With \name, the median 
slowdown\footnote{We define the ``slowdown'' of a request as its completion time divided by what its completion time would have been in an unloaded network. A slowdown of $1$ is optimal, and lower numbers represent better performance.} 
decreases from $1.62$ for Baseline to $1.08$ with \name: 33\% lower.

Furthermore, \name's performance is near to the benefits that could be achieved with Optimal.
Both \name and Optimal achieve an identical median slowdown of $1.08$.
Optimal provides better performance in the tail: the $99\%$ile slowdown is $4.46$ for ``optimal'' and $9.83$ for \name.
Note that the baseline achieves a $99\%$ile slowdown of $10.77$.
We next discuss the source of this higher tail FCT compared to Optimal.

\paragrapha{Necessity of scheduling} \an{move this text to earlier?}It is important to note that \name by itself is not necessarily a means of achieving low end-to-end delays, as promised by recent efforts in congestion control~\cite{copa, nimbus}, despite its use of these modern congestion control algorithms.

To see why this is the case, recall that \name does not modify the end-hosts: they continue to run the default Cubic congestion controller, which will probe for bandwidth until it observes loss.
Indeed, the packets end-host Cubic sends beyond those the link can transmit must queue somewhere in the network or be lost. Without \name, they get queued up at the bottleneck link.
With \name, they instead queue at the \inbox.
However, the congestion controller at \inbox must also additionally maintain some standing queue at the bottleneck link to make decisions effectively.
Therefore, we expect the overall end-to-end-delays to increase slightly.
As a result, \emph{solely} using a more sophisticated congestion control algorithm at the \name is unlikely to be the cause of significant reductions in the flow completion time.

This is why the tail FCT for \name does not match Optimal's.
We can see this by measuring the relative performance of using FIFO scheduling at the \name instead of the SFQ scheduling in Figure~\ref{fig:eval:best}.
The results are in Figure~\ref{fig:eval:fifo}. 
Unsurprisingly, the FCTs with FIFO scheduling at \name are \emph{worse}: with a median FCT of $173$ms, it is $18$\% higher than the baseline. 
\input{fifo-not-enough}

\subsection{Different Scheduling Policies}\label{s:eval:policies}

\input{lowdelays}
\paragrapha{Achieving Low Delays}
\name can configure its scheduling policy to achieve low end-to-end-delays.
In Figure~\ref{fig:eval:lowdelays}, we show that when we configure the scheduling policy at \inbox, we can change the RTTs component connections observe.
Note that due to the effect discussed above, using FIFO scheduling results in higher delays than the baseline case.

\paragrapha{Rate Stability}\label{s:eval:ratestable}

\begin{outline}
    \1 \name can improve the rate stability of component traffic
    \1 One such class of traffic is video streaming
\end{outline}

\paragrapha{Strict Priorities}

\begin{outline}
    \1 \name can support strict priorities for interactive traffic.
    \1 Experiment: backlogged iperf flows are in a lower priority queue and interactive ETG flows are in a high priority queue.
\end{outline}

%\radhika{need to further clean this.}

\subsection{Terminating TCP Connections}\label{s:eval:proxy}
An alternate implementation choice for a \name would use a TCP proxy to terminate connections at the \inbox. 
%\radhika{I assume we mention this in an earlier section? else we need a line or two about why this experiment is interesting.}
We consider the best-case outcome of terminating TCP connections --- the effective RTT observed by the end-to-end congestion controller running at the servers would decrease (since the proxy would acknowledge its segments much faster than the original receiver would), and it would grow its window rapidly.
A proxy-based design would then apply scheduling policy to component traffic (recall that we use SFQ).
We emulate this best-case scenario by modifying the end-hosts to maintain a constant congestion window of $450$ packets --- slightly larger than the BDP --- and increasing the queue lengths at the \inbox to hold these packets. The other aspects of \name remain unchanged.
The result is in Figure~\ref{fig:eval:proxy}.
%
\input{proxy}
%
\an{check this text}
For the short requests, terminating TCP connections does not yield additional benefits: with or without termination, they finish in a few RTTs and never leave slow start.
For medium-to-long requests, terminating TCP connections yields benefits since they no longer incur the penalty of window growth.

\input{robust}

\subsection{Offered load}\label{s:eval:offeredload}
Naturally, if a link is less congested, scheduling the packets that traverse it will have less benefit. Accordingly, as we reduce the offered load in this experiment, we expect the gains from scheduling to diminish. The result is in Figure~\ref{fig:eval:offeredload}. We reduce the offered load by removing persistently backlogged connections from the workload and generating individual requests at 50\% ($48$Mbps), 75\% ($72$Mbps) and 87.5\% ($84$Mbps) of the bottleneck link bandwidth.
At 87.5\% load, even without the load offered by a persistently backlogged connection, \name improves FCTs by \an{amount}. 
As the offered load decreases to 50\%, the benefit provided by \name decreases as well -- to \an{amount} at the 75th percentile.

\input{varying-offered-load}
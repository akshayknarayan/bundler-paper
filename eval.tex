\section{Evaluation}\label{s:eval}

%We have seen in Figure~\ref{fig:design:shift-bottleneck} that \name can move the network queues to the \inbox to gain control over scheduling. 
%Given that this is possible, what benefits can \name achieve, and where do they come from?
Given \name's ability to move the in-network queues to the \inbox (as shown earlier in Figure~\ref{fig:design:shift-bottleneck}), we now evaluate the following:
\begin{enumerate}
    \item Where do \name's performance benefits come from? We discuss this in the context of improving the flow completion times of \name's component flows. (\S\ref{s:eval:fct})
    \item Can \name enforce different scheduling policies? (\S\ref{s:eval:policies})
    \item Do \name's performance benefits hold across different scenarios? (\S\ref{s:robust:cross})
    %\item Under what conditions do \name's performance benefits hold? (\S\ref{s:robust:cross})
\end{enumerate}

\subsection{Understanding \name's Performance Benefits}\label{s:eval:fct}

\paragrapha{Experimental Setup}
We evaluate our implementation of \name (discussed in \S\ref{s:impl}) using network emulation via mahimahi~\cite{mahimahi}.
Mahimahi creates a Linux network namespace and applies the configured network condition (emulated delay or link bandwidth) to all packets that traverse it. 

There are three $8$-core machines in our setup: one machine is a sender, another is configured as a middlebox and runs a \inbox, and a third is the receiver. The \outbox runs on the same machine as the receiver, inside the same mahimahi network namespace as the receiving application. 
Our prototype requires that the sending application and the \inbox run on different machines, to prevent TCP small queues (TSQ) from interfering with the queuing behaviour that one would expect in a real deployment. 
%It is important for experiment fidelity to run the sending application on a different machine than the \inbox; otherwise, because the \inbox data-path uses tc (\S\ref{s:impl}), TCP small queues (TSQ) unrealistically avoids queueing.
Since our \outbox implementation uses \texttt{libpcap}, GRO would change the packets before they are delivered to the \outbox, which would cause inconsistent epoch boundary identification between the two boxes. We, therefore, disable TSO and GRO.
Throughout our experiments, CPU utilization on the machines remained below $10$\%.

% \an{somebody make sure this makes sense}\an{Since our prototype implementation uses the Linux kernel datapath, we disable TCP segmentation offload (TSO) and generic receive offload (GRO) on the machines to maintain a consistent view of packet headers between the \inbox and the \outbox.
% This is an artifact of our prototype implementation of the \outbox using \texttt{libpcap}: in a real deployment, the \outbox would have the same view of the packets as the \inbox.}


A many-threaded client generates requests from a request size CDF drawn from an Internet core router~\cite{caida-dataset}, and assigns them to one of $200$ server processes on the traffic generator.
The workload is heavy-tailed: 97.6\% of requests are 10KB or shorter, and the largest 0.002\% of requests are between $5$MB and $100$MB.
Each server then sends the requested amount of data to the client and we measure the flow completion time of each such request. \reword{In addition, the server also generates a persistently backlogged connection to the client.}
%We use multiple server processes to emulate multiple machines behind the \name at the customer's edge.

Unless otherwise specified, we use the following settings. The link bandwidth at the mahimahi link is set to 96Mbps, and the one-way delay is set to 25ms. The requests result in an offered load of 84Mbps. The endhost runs Cubic~\cite{cubic} congestion controller, and the \inbox runs Copa~\cite{copa}. Each experiment is comprised of $100,000$ requests sampled from this distribution, across $10$ runs each with a different random seed. 

%that models a background large data transfer which would normally fill the bottleneck queue.
%
%\radhika{another title for this section, or some preceding explanation on what each section is about}
%
%\cut{Note that it is important to use many server processes so that the effect of head-of-line blocking on the client requests is limited.
%Indeed, all the CDFs presented in this section have a ``knee'' where the effects of head-of-line blocking become apparent.
%}
%\radhika{when the graphs are presented, explain the `knee' as an artifact of our expts caused due to some HoL blocking. Btw, are you sure that's why you have the knee?}

\input{overview-benefits}
\newcommand{\baseline}{Status Quo\xspace}
\newcommand{\optimal}{In-Network\xspace}

\paragrapha{Comparison with \baseline and \optimal Scheduling} 
We first present results for a simplified scenario without any cross-traffic, i.e. all traffic traversing through the network is generated by the same customer and is, therefore, part of the same bundle. 
This scenario highlights the benefits of using \name when the congestion on the bottleneck link in the network is self-inflicted. We explore the effects of congestion due to other cross-traffic in \S\ref{s:robust:cross}.

%On a $96$Mbps link, we generate  from the CDF described above.

In this section, we evaluate the benefits provided by doing fair queuing at the \name, and use median slowdown as our metric, where the ``slowdown'' of a request as its completion time divided by what its completion time would have been in an unloaded network. A slowdown of $1$ is optimal, and lower numbers represent better performance.

We evaluate three configurations: 
(i) The ``\baseline'' configuration represents the status quo: the \inbox simply forwards packets as it receives them, and the mahimahi bottleneck uses FIFO scheduling.
(ii) The ``\optimal'' configuration deploys fair queueing\footnote{
We implement this scheme by modifying mahimahi (our patch comprises $171$ lines of C++) to add a packet-level fair-queueing scheduler to the bottleneck link.}
at the mahimahi bottleneck. 
Recall from \S\ref{s:intro} that this configuration is not enforced in practice.
%: it would force all customers --- who may desire diverging scheduling policies --- to use the same scheduler.
(iii) The default \name configuration, that uses Copa~\cite{copa} for rate control and the stochastic fair queueing~\cite{sfq} scheduling policy at the \inbox.
%(iv) Finally, perform a factor analysis for \name, we also evaluate a configuration where \name does FIFO scheduling. 



Figure~\ref{fig:eval:best} presents the results with the above experiment setup. 
As is expected from the use of fair queuing, \name is able to achieve significant reductions in the slowdown for short flows, followed by some reduction for medium-sized flows, and similar performance for large flows. 
%\name, using fair queueing, significantly improves the slowdown for short ($<$10KB) flows, while the medium
The median slowdown
%\footnote{We define the ``slowdown'' of a request as its completion time divided by what its completion time would have been in an unloaded network. A slowdown of $1$ is optimal, and lower numbers represent better performance.} 
decreases from \overviewBenefitsBaselineMedian 
for Baseline to \overviewBenefitsBundlerMedian 
with \name: \overviewBenefitsBundlerMedianImprovement
lower.

Furthermore, \name's performance is close to \optimal.
Both \name and \optimal achieve an identical median slowdown of \overviewBenefitsBundlerMedian.
\optimal provides better performance in the tail: the $99\%$ile slowdown is \overviewBenefitsOptimalTail for ``\optimal'' and \overviewBenefitsBundlerTail for \name.
Meanwhile, the \baseline achieves a $99\%$ile slowdown of \overviewBenefitsBaselineTail.
%We next discuss the source of this higher tail FCT compared to \optimal.

\paragrapha{Necessity of scheduling} It is important to note that \name by itself (i.e. without a suitable scheduling policy) is not a means of achieving improved performance. 
%as promised by recent efforts in congestion control~\cite{copa, nimbus}, 
despite doing aggregated congestion control across all flows in a bundle.
To see why this is the case, recall that \name does not modify the end-hosts: they continue to run the default Cubic congestion controller, which will probe for bandwidth until it observes loss.
Indeed, the packets end-host Cubic sends beyond those that the link can transmit must be queued somewhere in the network or get dropped. Without \name, they get queued up at the bottleneck link. With \name, they instead just get queued up at the \inbox. In addition, the congestion controller at \inbox also maintains a small standing queue at the bottleneck link (which can be seen in Figure~\ref{fig:design:shift-bottleneck}) to avoid under-utilization, which increases the end-to-end-delays slightly. Therefore, doing the FIFO scheduling at the \name, as is done by the \baseline, results in slightly worse performance (shown in Figure~\ref{fig:eval:fifo}).
%As a result, \emph{solely} using a more sophisticated congestion control algorithm at the \name is unlikely to be the cause of significant reductions in the flow completion time.

\input{fifo-not-enough}
% This is why the tail FCT for \name does not match \optimal's.
% We can see this by measuring the relative performance of using FIFO scheduling at the \name instead of the SFQ scheduling in Figure~\ref{fig:eval:best}.
% The results are in Figure~\ref{fig:eval:fifo}. 
% Unsurprisingly, the FCTs with FIFO scheduling at \name are \emph{worse}: with a median slowdown of \overviewBenefitsFifoMedian, it is \overviewBenefitsFifoWorse higher than the \baseline. 

\subsection{Different Scheduling Policies}\label{s:eval:policies}
Per \S\ref{s:impl}, our \name implementation can implement any scheduling discipline available in Linux. We demonstrate that \name can achieve low packet delay, strict prioritization, and rate fairness.

\paragrapha{Achieving Low Delays}
\name uses fq-codel~\cite{fq-codel}, which adds ECN marks to packets in fair-queue buckets which exceed a threshold. 
As a result, endhosts cut their windows earlier than otherwise, and thus achieve low end-to-end-delays.
We measure the distribution of RTTs endhost connections observe with \name and \baseline.
\input{lowdelays}
Because it controls endhost packet transmissions, fq-codel achieves \delaysImprovement lower median packet delay.
%Note that due to the effect discussed above, using FIFO scheduling results in higher delays than the \baseline case.

\paragrapha{Strict Prioritization}\label{s:eval:strictprio}
Using a priority scheduler at \name can improve FCTs for a higher-priority class of traffic transitting \name over a lower-priority class. 
\input{strict-prio}
Furthermore, when compared to fair queueing in this scenario, prioritization achieves \strictPrioTailImprovementOverFq lower $99$\%ile FCT for the prioritized traffic class.

\input{waterfall}
\paragrapha{Rate Fairness and Stability}\label{s:eval:waterfall}
In Figure~\ref{fig:eval:waterfall}, we show that \name, when configured with a fair scheduler, converges to fair and stable rates faster than the \baseline.

\cut{
\input{video-exp}
\paragrapha{Rate Stability}\label{s:eval:ratestable}
\fc{come back to this}
In Figure~\ref{fig:eval:video}, we run a persistently backlogged flow over a 24Mbps link and then
after 3 seconds start a client attempting to stream a 4k video from a server that supports adaptive
bitrate selection. Without \name (a), the video stream experiences highly variable throughput and 
takes 30 seconds to converge to a fair share of the link. In contrast, with \name (b), the video
stream converges to its fair share within 2 seconds and is able to maintain that rate for the
entirety of the stream. This stability provides the best scenario for the ABR algorithm to select
the highest possible bitrate and thus maximize QoE.
}

\subsection{Terminating TCP Connections}\label{s:eval:proxy}
An alternate implementation of a \name could terminate TCP connections at the \inbox. 
%\radhika{I assume we mention this in an earlier section? else we need a line or two about why this experiment is interesting.}
We consider the best-case outcome of terminating TCP connections --- the RTT the end-to-end congestion controller observed would decrease (since the proxy would acknowledge its segments much faster than the original receiver would), and it would therefore grow its window rapidly.
A proxy-based design could then apply a scheduling policy to the acknowledged, but yet unsent traffic from terminated connections.
To emulate a best-case scenario for termination, we modify the end-hosts to maintain a constant congestion window of $450$ packets --- slightly larger than the BDP --- and increase the buffering at the \inbox to hold these packets. 
The other aspects of \name, including scheduling, remain unchanged.
The result is in Figure~\ref{fig:eval:proxy}.
%
\input{proxy}
%
\an{check this text}
For the short requests which never leave TCP slow start, terminating TCP connections does not yield additional benefits: with or without termination, they finish in a few RTTs.
For medium-to-long requests, terminating TCP connections yields benefits since they no longer incur the penalty of window growth.

\input{robust}

\subsection{Offered load}\label{s:eval:offeredload}
Naturally, if a link is less congested, scheduling the packets that traverse it will have less benefit. Accordingly, as we reduce the offered load in this experiment, we expect the gains from scheduling to diminish. The result is in Figure~\ref{fig:eval:offeredload}. We reduce the offered load by removing persistently backlogged connections from the workload and generating individual requests at 50\% ($48$Mbps), 75\% ($72$Mbps) and 87.5\% ($84$Mbps) of the bottleneck link bandwidth.
At 87.5\% load, even without the load offered by a persistently backlogged connection, \name improves FCTs by \an{amount}. 
As the offered load decreases to 50\%, the benefit provided by \name decreases as well -- to \an{amount} at the 75th percentile.

\input{varying-offered-load}

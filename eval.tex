\section{Evaluation}\label{s:eval}

%We have seen in Figure~\ref{fig:design:shift-bottleneck} that \name can move the network queues to the \inbox to gain control over scheduling. 
%Given that this is possible, what benefits can \name achieve, and where do they come from?
Given \name's ability to move the in-network queues to the \inbox (as shown earlier in Figure~\ref{fig:design:shift-bottleneck}), we now explore:
\begin{enumerate}[leftmargin=15pt]
    \item Where do \name's performance benefits come from? We discuss this in the context of improving the flow completion times of \name's component flows. (\S\ref{s:eval:fct})
    \item Do \name's performance benefits hold across different scenarios? (\S\ref{s:robust:cross})
    \item Can \name work with different congestion control algorithms (\S\ref{s:eval:cc})?
    \item Are \name's core ideas still applicable with other design decisions? (\S\ref{s:eval:proxy})
    \item Is \name's heuristic (\S\ref{s:queue-ctl:ecmp}) for detecting imbalanced multipath scenarios robust? (\S\ref{s:eval:ecmp})  
    \item Can \name effectively control the queues on real Internet paths? (\S\ref{s:eval:realworld})
\end{enumerate}

\subsection{Experimental Setup}\label{s:eval:setup}

We use network emulation via mahimahi~\cite{mahimahi} to evaluate our implementation of \name in a controlled setting; we present results on real Internet paths in \S\ref{s:eval:realworld}.
There are three $8$-core Ubuntu 18.04 machines in our emulated setup: (1) runs a sender, (2) runs a \inbox, and (3) runs both a \outbox and a receiver.
% The \outbox runs on the same machine as the receiver.
We disable both TCP segmentation offload (TSO) and generic receive offload (GRO) as they would change the packet headers in between the \inbox and \outbox, which would cause inconsistent epoch boundary identification between the two boxes.
% Since our \outbox implementation uses \texttt{libpcap}, generic receive offload (GRO) would change the packets before they are delivered to the \outbox, which would cause inconsistent epoch boundary identification between the two boxes. We, therefore, disable TCP segmentation offload and generic receive offload.
Nevertheless, throughout our experiments CPU utilization on the machines remained below $10$\%.

Unless otherwise specified, we emulate the following scenario.
A many-threaded client generates requests from a request size CDF drawn from an Internet core router~\cite{caida-dataset} and assigns them to one of $200$ server processes.
The workload is heavy-tailed: 97.6\% of requests are 10KB or shorter, and the largest 0.002\% of requests are between $5$MB and $100$MB.
Each server then sends the requested amount of data to the client and we measure the flow completion time of each such request. 
The link bandwidth at the mahimahi link is set to 96Mbps, and the RTT is set to 50ms. The requests result in an offered load of 84Mbps. 

The endhost runs Cubic~\cite{cubic}, and the \inbox runs Copa~\cite{copa} (we test other schemes in \S\ref{s:eval:cc}) with Nimbus~\cite{nimbus-arxiv} for cross traffic detection. 
Each experiment is comprised of 1,000,000 requests sampled from this distribution, across 10 runs each with a different random seed.
%\an{new, short version of ``applying different scheduling policies''}:
The \inbox schedules traffic using stochastic fair queueing~\cite{sfq} in our experiments. 


\subsection{Understanding Performance Benefits}\label{s:eval:fct}

We first present results for a simplified scenario without any cross-traffic, \ie all traffic traversing through the network is generated by the same customer and is, therefore, part of the same bundle. 
This scenario highlights the benefits of using \name when the congestion on the bottleneck link in the network is self-inflicted. We explore the effects of congestion due to other cross-traffic in \S\ref{s:robust:cross}.

\input{overview-benefits}
\newcommand{\baseline}{Status Quo\xspace}
\newcommand{\optimal}{In-Network\xspace}

\Para{Using \name for fair queueing}
In this section, we evaluate the benefits provided by doing fair queuing at the \name, and use median slowdown as our metric, where the ``slowdown'' of a request is its completion time divided by what its completion time would have been in an unloaded network. A slowdown of $1$ is optimal, and lower numbers represent better performance.

We evaluate three configurations: 
(i) The ``\baseline'' configuration represents the status quo: the \inbox simply forwards packets as it receives them, and the mahimahi bottleneck uses FIFO scheduling.
(ii) The ``\optimal'' configuration deploys fair queueing
at the mahimahi bottleneck\footnote{
We implement this scheme by modifying mahimahi (our patch comprises $171$ lines of C++) to add a packet-level fair-queueing scheduler to the bottleneck link.}. 
Recall from \S\ref{s:intro} that this configuration is not deployable.
%: it would force all customers --- who may desire diverging scheduling policies --- to use the same scheduler.
(iii) The default \name configuration, that uses stochastic fair queueing~\cite{sfq} scheduling policy at the \inbox, and (iv) Using \name with FIFO (without exploiting scheduling opportunity).
%\radhika{flip the order for in-network and \name in the figure to be consistent with the above order.}

Figure~\ref{fig:eval:best} presents our results. 
The median slowdown (across all flow sizes) decreases from \overviewBenefitsBaselineMedian 
for Baseline to \overviewBenefitsBundlerMedian 
with \name, \overviewBenefitsBundlerMedianImprovement
lower. 
\optimal's median slowdown is a further 15\% lower then \name: \overviewBenefitsOptimalMedian.
Meanwhile, in the tail, \name's $99\%$ile slowdown is \overviewBenefitsBundlerTail, which is 48\% lower than the \baseline's \overviewBenefitsBaselineTail. \optimal's $99\%$ile slowdown is \overviewBenefitsOptimalTail.

\paragrapha{Using \name for other policies} We additionally evaluated other scheduling and queue management policies with \name. We omit detailed results for brevity, and present a few highlights. With FQ-CoDel~\cite{fq-codel}, \name can achieve 97\% lower median end-to-end RTTs and 89\% lower 99\%ile RTTs.  By strictly prioritizing one traffic class over another, \name results in 65\% lower median FCTs for the higher-priority class. 

\cut{
\subsection{Applying Different Scheduling Policies}\label{s:eval:policies}
%Per \S\ref{s:impl}, our \name implementation can implement any scheduling discipline available in Linux. 

\an{Reduce this subsection to ~one paragraph and roll into 7.2}

In addition to improving flow completion times, \name can achieve low packet delay, perform strict prioritization, and rate fairness.

\paragrapha{Achieving Improved Flow Completion Times} \S\ref{s:eval:fct} shows how enabling SFQ at the \name improves the median slowdown by \overviewBenefitsBundlerMedianImprovement.

\paragrapha{Achieving Low Packet Delays}
We enable CoDel~\cite{fq-codel} at the \inbox to lower the packet delays, and test it for a single large backlogged flow using the setup described in \S\ref{s:eval}.
CoDel adds ECN marks to packets in fair-queue buckets which exceed a queue length threshold. 
As a result, endhosts cut their windows earlier, thus reducing their self-inflicted delay within their fair-queue bucket.
We measure the resulting distribution of RTTs seen by the endhost connections with \name and \baseline in Table~\ref{t:eval:codel}.
\input{lowdelays}
As is expected from CoDel, \name in this experiment, achieves \delaysImprovement lower median packet delay than \baseline.

\paragrapha{Strict Prioritization}\label{s:eval:strictprio}
We uniformly divide the web request distribution described in \S\ref{s:eval:setup} into two equally sized classes, one of which is given a higher priority over the other. 
%\radhika{what's the division ratio?}\an{specified equal ratio}  
The results are presented in Table~\ref{t:eval:prio}.
%of traffic transitting \name over a lower-priority class. 
\input{strict-prio}
Using a priority scheduler (we use the \texttt{pfifo\_fast} qdisc) at \inbox improves the flow completion times for the higher-priority class compared to \baseline.
Furthermore, prioritization achieves \strictPrioTailImprovementOverFq lower $99$\%ile FCT for the higher priority traffic class, when compared to using fair queueing.

\input{waterfall}
\paragrapha{Rate Fairness and Stability}\label{s:eval:waterfall}
We next use our default SFQ scheduler to achieve fairness and rate stability. We start three backlogged flows at different times (0s, 15s, and 30s). Figure~\ref{fig:eval:waterfall} shows that \name converges to fair and stable rates faster than the \baseline.

\input{video-exp}
\paragrapha{Rate Stability}\label{s:eval:ratestable}
\fc{come back to this}
In Figure~\ref{fig:eval:video}, we run a persistently backlogged flow over a 24Mbps link and then
after 3 seconds start a client attempting to stream a 4k video from a server that supports adaptive
bitrate selection. Without \name (a), the video stream experiences highly variable throughput and 
takes 30 seconds to converge to a fair share of the link. In contrast, with \name (b), the video
stream converges to its fair share within 2 seconds and is able to maintain that rate for the
entirety of the stream. This stability provides the best scenario for the ABR algorithm to select
the highest possible bitrate and thus maximize QoE.
}

\paragrapha{Aggregate congestion control is not enough} It is important to note that \name's congestion control by itself (\ie running FIFO scheduling) is not a means of achieving improved performance. 
To see why this is the case, recall that \name does not modify the endhosts: they continue to run the default Cubic congestion controller, which will probe for bandwidth until it observes loss.
Indeed, the packets endhost Cubic sends beyond those that the link can transmit must queue somewhere in the network or get dropped. 
Without \name, they queue at the bottleneck link;
with \name, they instead queue at the \inbox. 
In addition, the delay-based congestion controller at \inbox also maintains a small standing queue at the bottleneck link (which can be seen in Figure~\ref{fig:design:shift-bottleneck}) to avoid under-utilization, which increases the end-to-end-delays slightly. 
Therefore, doing the FIFO scheduling at the \name, as is done by the \baseline, results in slightly worse performance.

% cut, combine with overview-benefits
%\input{fifo-not-enough}

\input{robust}

\cut{
\begin{Appendix}
\section{Varying Offered Load}\label{s:eval:offeredload}
Naturally, if a link is less congested, scheduling the packets that traverse it will have less benefit. Accordingly, as the offered load is reduced, we would expect the gains from scheduling to diminish. 
We now use the web request distribution described in \S\ref{s:eval:setup} to generate a load of 50\% ($48$Mbps), 75\% ($72$Mbps) and 87.5\% ($84$Mbps) of the bottleneck link bandwidth. Our results in Figure~\ref{fig:eval:offeredload} show that as the offered load is decreased, the benefits of \name reduce. This is because if a link is less congested, scheduling the packets that traverse it will have less benefit.
% At 87.5\% load, even without the load offered by a persistently backlogged connection, \name improves FCTs by \an{amount}. 
% As the offered load decreases to 50\%, the benefit provided by \name decreases as well -- to \an{amount} at the 75th percentile.
\end{Appendix}
}

\subsection{Terminating TCP Connections}\label{s:eval:proxy}

Although our \name prototype does not terminate connections (as discussed in \S\ref{s:design:whichcc}), we note that terminating connections does provide one key advantage: the end-to-end congestion controller will observe a smaller RTT, since the proxy can acknowledge its segments much faster than the original receiver. 
This enables rapid window growth at the endhosts.
While there are, of course, operational concerns with managing the resulting queue, it does provide additional scheduling opportunities as well as faster ramp-up for midsized connections.

How much benefit, then, could a proxy-based \name provide?
To evaluate this, we emulate an idealized TCP proxy by modifying the endhosts to maintain a constant congestion window of $450$ packets---slightly larger than the bandwidth-delay product in our setup---and increasing the buffering at the \inbox to hold these packets. 
The other aspects of \name remain unchanged.
The result is in Figure~\ref{fig:eval:proxy}. 

For the short requests which never leave TCP slow start, terminating TCP connections does not yield additional benefits: with or without termination, they finish in a few RTTs.
For medium-to-long requests, terminating TCP connections yields additional benefits since they no longer incur the penalty of window growth.
Therefore, a site may benefit from proxying TCP connections at \name if its traffic pattern contains many medium-sized flows which benefit from fast ramp-up.

\input{proxy}

\subsection{Multipath Detection}\label{s:eval:ecmp}

% TODO: after deadline should also run experiment where there are > 1 queues but no imbalance, and then suddenly a large flow comes in and creates imbalance, then leaves, heuristic should go above threshold and come back down correspondingly 
As described in \S\ref{s:queue-ctl:ecmp}, when the ratio of out-of-order to in-order measurements is above a certain threshold, it indicates that \name's component flows are likely traversing multiple imbalanced paths. To evaluate the extent to which this heuristic corresponds with imbalance, we re-run the emulation experiment from Figure~\ref{fig:eval:bigexp} for a variety of network conditions (bottleneck bandwidth ranging from 12 to 96 Mbps, end-to-end RTTs ranging from 10 to 300 ms, and bottleneck load-balancing from 1 to 32 paths) and consider the average value reported by the heuristic over each experiment. The maximum value reported across all experiments with a single path was 0.4\%, while the minimum value reported across all experiments with 2-32 paths was 20\%, two orders of magnitude greater. Thus, this heuristic provides a very clear separation between single and multiple path scenarios and a simple threshold is sufficient. 

% Motivated by these results, we use 0.05 as a threshold for our real-world experiments in the following section. We classified each path as single or multipath by whether or not the component flows experienced different min-RTTs (which we can do in this experiment since we control the flows but \name would not be able to do itself), and found that our chosen threshold correctly distinguished between single and multiple path in all cases. 

% \begin{figure}
%     \centering
% \includegraphics[width=\maxwidth]{figure/eval:ecmp:ratio} 
%     \caption{The queue imbalance heuristic as a function of number of queues. }
%     \label{fig:eval:ecmp:ratio}
% \end{figure}
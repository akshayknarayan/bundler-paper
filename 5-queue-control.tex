%\section{Coping with Cross Traffic}
\section{\cut{Handling }Unfavorable Conditions}\label{s:queue-ctl}

Recall from \S\ref{s:deploy} that \name can reliably shift queue build up from the bottleneck to itself when, (a) the cross-traffic is not buffer-filling, and (b) all of its component traffic shares the same bottleneck in the network.
In practice, either of these conditions may break. 
%not always hold on the real Internet, and they may change over time.
In this section, we describe how \name can re-use the same measurements from \S\ref{s:measurement} to detect when these conditions do not hold. In such cases, \name (temporarily) disables its rate limiting (falling back to status-quo performance) until favorable conditions arise again. 
%Thus, even in the worst case \name will not degrade performance (relative to the status quo without \name).
% simply degrades to status quo performance.
% Of course, in real Internet conditions, there may be scenarios where these conditions do not hold.
% In these cases, \name gracefully degrades to status quo performance. 

\subsection{Buffer-Filling Cross Traffic}
\label{s:buffer-filling}

%As \name uses a delay-minimizing congestion control algorithm to shift queue build up from the bottleneck to the \inbox.
%A well-known property of delay-minimizing congestion control algorithms is that when competing with traditional loss-based controllers, they lose throughput~\cite{copa}.

%Therefore, it is important for congestion controllers at the \inbox to detect the presence of such cross-traffic and disable the use of delay-control in these situations.
%Once the buffer-filling cross traffic leaves, the \inbox should resume delay control.

It is well known that delay-based congestion control algorithms (as \name uses) lose throughput when competing with traditional loss-based controllers~\cite{copa}. Therefore, in order to compete fairly with buffer-filling cross-traffic, \name must first detect the presence of such traffic and disable its use of a delay-based controller.  
%\paragrapha{Competing Fairly}

Prior work (Nimbus~\cite{nimbus-arxiv}) presents a method for detecting buffer-filling cross traffic, that \name employs.
\footnote{\cite{nimbus-arxiv} includes a detailed evaluation of Nimbus' accuracy of detecting buffer-filling cross traffic and speed of switching between the two modes, using both emulated and real-world experiments. Bundler's use of Nimbus does not impact its accuracy or speed of switching.}
But what exactly should the \inbox do when it detects this condition?
A naive approach might replace the delay-based control at \name with a loss-based buffer-filling congestion control such as Cubic. 
However, with this approach, \name must measure the number of flows in a bundle, and should compete as aggressively in proportion to achieve an aggregate throughput equivalent to status quo~\cite{multcp}. On high-performance datapaths, it may be difficult to measure this number~\cite{heavy-hitters}.

We propose a simpler solution.
Since bundles comprise of traditional end-host connections, with their own congestion controllers, \name can simply \emph{let the traffic pass}, \ie increase the pacing rate at the \inbox to stop controlling queues. Then, the end-host congestion control loop will naturally compete fairly with the aggressive cross traffic, just as traffic in the Internet does today.
%Copa is not compatible with this approach because its TCP-compatible mode is designed for the context of a single connection. Therefore, we must use Nimbus~\cite{nimbus} for cross traffic detection.

%\paragrapha{Active Probing}
This brings us to the next natural question: How can the \inbox know it is safe to resume delay-control (and scheduling) after disabling it?
%It is important to distinguish between \emph{self-inflicted} queueing delay and queueing delay due to cross traffic.
%When the queueing delay is purely self-inflicted, it is safe to resume control over the queues at the \inbox.
One approach is to send passive probes along the network to detect the presence/absence of queuing. However, such passive measurements cannot distinguish between the self-inflicted queuing due to \name's traffic and the queuing due to cross-traffic. If the bottleneck queue is entirely self-inflicted, it is safe (and desirable) to resume delay-control and scheduling.
%Passive probing is insufficient to determine this state, since passive measurements of the bottleneck will be identical in the case of self-inflicted queueing and queueing driven by cross traffic.
Therefore, it is important to \emph{actively probe}, that is, change the rate of bundled traffic and observe the response of the cross traffic. 
This is what the Nimbus mechanism does.
At a high level,
Nimbus sinusoidally varies the sending rate $r(t) = A sin(\frac{4\pi{}t}{T})$ during the up-pulse, where $A$ is the pulse amplitude (set to one-fourth of the estimated bottleneck bandwidth) and $T$ is the pulse duration, and measures the cross traffic's response in the frequency domain.
The \inbox can use the Nimbus algorithm to detect when to relinquish control over the queue by interposing this sending pattern over the delay-controller's rate decisions.
However, if the \inbox entirely drains its queues into the network, it will no longer be possible for Nimbus to overlay pulses onto the traffic pattern, and it will be unable to determine the nature of the cross traffic.
Practically, this would mean that once \inbox switches to compete with cross traffic, it would never gain the information necessary to switch back.

Instead, to support active probing while also letting the traffic pass, the \inbox must maintain some queueing.
How many packets should this be? The \inbox should be able to generate enough packets for a Nimbus up-pulse, \ie the area under the up-pulse curve: 
$A \int_0^{\frac{T}{4}} \sin(\frac{4\pi{}t}{T}) dt = \frac{AT}{2\pi}$.
From Nimbus, we use $T = 0.2$ seconds and $A$ is as above, one-fourth the bottleneck bandwidth. By Little's law, we can calculate the amount of extra queueing: $\frac{T}{8\pi}$, or $8$ms.
We thus configure the \inbox to hold back $10$ms of queueing for active probing; the additional queueing is a cushion against input variance.
Note that this extra queueing is in addition to queueing in the network. As a result, the end-to-end Cubic connections will see mild RTT inflation.
In \S\ref{s:robust:cross} we show that this effect is not large; \name still achieves performance comparable to the status quo.

How should we achieve this target queueing delay? 
This problem is similar to the role of the PIE AQM mechanism~\cite{pie}, which also seeks to maintain a queueing delay target.
Correspondingly, we design a PI controller at the \inbox as part of the fairness control module. 
It overlays a rate $r$ corresponding to $\dot{r}(t) = \alpha (q - q_T) + \beta (\dot{q})$, where $q$ is the queue size and $q_T$ is the target queue size computed above.
We pick $\alpha = 10$ and $\beta = 10$ by solving for a convergence time of one RTT (Appendix~\ref{app:derive-ab}).
% as per \S\ref{s:qctl:pi}.

\subsection{Imbalanced Multipathing}\label{s:queue-ctl:ecmp}
Since a \bundle contains many component connections, a load balancer may send them along different paths. If the load along different paths is well-balanced, \name will accurately treat a load-balanced bottleneck link as a single link with the aggregate rate of each sub-link. However, when the load along different paths is imbalanced, the series of measurements at the \name will be a random sampling of the different paths, which would confuse the delay-control algorithm and cause it to perform poorly. Fortunately, such cases are straight-forward to detect with our measurement technique. 
More specifically, load imbalance will result in many epoch measure packets arriving out-of-order at the \outbox (whenever epoch packet $i$ happens to traverse a path with a larger delay than epoch packet $i+1$), and consequently, out-of-order ``congestion ACKs'' at the \inbox.  Figure~\ref{fig:queue-ctl:ecmp:motivation} shows this effect for an emulated imbalance scenario. Therefore, we use the fraction of epoch measurement packets that arrive out-of-order as an indicator of load imbalance due to multipathing. 
If this number is small, the links are roughly balanced and \name will operate correctly.
If it is large, it indicates load imbalance, in which case \name's rate-control would work incorrectly. 
%While we believe that it is possible to design a rate controller that distinguishes RTT samples from different paths, and computes appropriate aggregate rates, we leave this for future research, and instead adopt a simpler
%the delay controller will make erratic adjustments as it briefly observes high RTTs, and may lose throughput as a result.
Therefore, if the reordering level is above 5\%, we disable rate control and revert to status quo performance. We evaluate this approach (and justify our chosen threshold) in \S\ref{s:eval:ecmp}.
%In this case, the measurement sub-system (\S\ref{s:measurement}) would report erratic measurements corresponding in turn to each of the possible paths.
% To see why, consider that \name's epoch-based rate measurements do not distinguish the path the packets take, and instead consider only the number of packets sent and received in an epoch.
% Thus, \name will accurately measure a load-balanced bottleneck link as a single link with the aggregate rate of each sub-link.
% However, for RTT measurements, the link with the lowest RTT will be over-sampled because the first epoch feedback packet to arrive will be from this link.
% So, \name will not detect building queues in other links.
% In most cases, this is not fatal: the delay controller might yield too much queue to the bottleneck, causing \name to miss out on performance improvements.
% We leave the design of a delay controller that does not over-react to conflicting measurements, and thus further optimizes traffic-control ability, to future work.

%In extreme cases, there may be persistent imbalance in the level of queueing. 

\begin{figure}
    \centering
\includegraphics[width=\maxwidth]{figure/ecmp_delay.pdf} 
\vspace{2pt}
\caption{(Top) True delay for all packets of \name's component flows based on which of 4 load-balanced paths they traversed (unknown to \name). (Bottom) Delay measurements observed by Bundler, colored based on whether they were derived from an in-order or out-of-order epoch packet. Bundler's measurements cannot distinguish how many paths there are, but the relative number of out-of-order measurements is enough to clearly indicate the presence of multiple RTT-imbalanced paths.}
\label{fig:queue-ctl:ecmp:motivation}
\end{figure}

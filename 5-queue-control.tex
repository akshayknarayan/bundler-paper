%\section{Coping with Cross Traffic}
\section{\cut{Handling }Unfavorable Conditions}\label{s:queue-ctl}

Recall from \S\ref{s:deploy} that \name can reliably shift queue build up from the bottleneck to itself when, (a) the cross-traffic is not buffer-filling, and (b) all of its component traffic shares the same bottleneck in the network.
In practice, either of these conditions may break. 
%not always hold on the real Internet, and they may change over time.
In this section, we describe how \name can re-use the same measurements from \S\ref{s:measurement} to detect when these conditions do not hold. In such cases, \name (temporarily) disables its rate limiting (falling back to status-quo performance) until favorable conditions arise again. 
%Thus, even in the worst case \name will not degrade performance (relative to the status quo without \name).
% simply degrades to status quo performance.
% Of course, in real Internet conditions, there may be scenarios where these conditions do not hold.
% In these cases, \name gracefully degrades to status quo performance. 

\subsection{Buffer-Filling Cross Traffic}
\label{s:buffer-filling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% As described in \S\ref{4.3}, \name uses a delay-based congestion control algorithm to control queues in the middle of the network. 
It is well known that delay-based congestion control algorithms (as \name uses) lose throughput when competing with buffer-filling algorithms~\cite{copa}. 
To prevent this, \name utilizes prior work, Nimbus~\cite{nimbus-arxiv}, which provides a mechanism for detecting the presence of buffer-filling\footnote{In particular, Nimbus detects ``elastic'' cross-traffic~\cite{nimbus-arxiv}, a superset of buffer-filling traffic. \cite{nimbus-arxiv} provides an explanation of this distinction
and a detailed evaluation of Nimbus' accuracy of detecting elastic cross traffic and speed of switching between the two modes, using both emulated and real-world experiments. \name{}'s use of Nimbus does not impact its accuracy or speed of switching.} 
cross traffic, and proposes temporarily switching to a buffer-filling scheme to compete fairly whenever such cross traffic is present.
At a high-level, the detection mechanism works as follows: given a desired sending rate $r(t)$ (an input from an underlying congestion control algorithm), Nimbus superimposes an asymmetric sinusoid onto $r(t)$ to compute the actual sending rate. Then, it monitors the ACK receive rate in the frequency domain; the sinusoidal variations in the sending rate will be visible in the receive rate only if some buffer-filling cross traffic is sharing the same bottleneck queue. 

What exactly should the \inbox do when it detects buffer-filling traffic? Using a buffer-filling scheme for the bundle as in Nimbus would be fraught: since a bundle is comprised of many individual flows, the \inbox would need to know the number of flows in the bundle to know how aggressively it should compete in order to receive its fair share (as in the status quo)~\cite{multcp}. 
This number may vary significantly over time and would be difficult to measure, especially on high-performance datapaths~\cite{heavy-hitters}.

Instead, we propose a simpler solution.
Since each connection in a bundle is already employing its own congestion controller, \name{} can simply \emph{let the traffic pass}, \ie{} increase the pacing rate at the \inbox to stop controlling queues.
Then, the end-host congestion control loops will naturally compete fairly with the buffer-filling cross traffic, just as they would without \name{}.

However, letting the traffic pass creates a new challenge. 
To determine when it is safe to resume delay-control while in the traffic-passing mode, Nimbus requires a superimposed pulse in both modes. If we naively let the traffic pass, the \inbox queue would never build. As a result, there would not be sufficient packets in the queue to perform the rate increase for the up-pulse. 
Without the up-pulse, once the \inbox{} switched to the buffer-filling mode, it would not be able to gather sufficient information to switch back to delay-control mode once the buffer-filling cross traffic subsided.

%\paragrapha{Active Probing}
% This brings us to the next natural question: how can the \inbox know when it is safe to resume delay-control (and scheduling) after disabling it?
%It is important to distinguish between \emph{self-inflicted} queueing delay and queueing delay due to cross traffic.
%When the queueing delay is purely self-inflicted, it is safe to resume control over the queues at the \inbox.
% One approach is to send passive probes along the network to detect the presence/absence of queuing. However, such passive measurements cannot distinguish between the self-inflicted queuing due to \name's traffic and the queuing due to cross-traffic. If the bottleneck queue is entirely self-inflicted, it is safe (and desirable) to resume delay-control and scheduling.
%Passive probing is insufficient to determine this state, since passive measurements of the bottleneck will be identical in the case of self-inflicted queueing and queueing driven by cross traffic.
% Therefore, it is important to \emph{actively probe}, that is, change the rate of bundled traffic and observe the response of the cross traffic. 
% This is exactly what the Nimbus mechanism does.
% At a high level,
% TODO: r(t) is the desired sending rate, A sint is super-imposed on top
% r(t) is the rate you are trying 
% something calculates r(t), calculate based on sending and receiving rate, using e.g. basic delay rule  
% now we are trying to deicde, when we want to let the pas
% we are trying to figure out the change to r(t) from the basic delay rule
% if its too large you'll converge too quickly and cant superimpose sinusoid on top of it
% given a desired sending rate $r(t)$, Nimbus superimposes an asymmetric sinusoid on top of $r(t)$, and measures the cross traffic's response in the frequency domain.
% Nimbus sinusoidally varies the sending rate $r(t) = A sin(\frac{4\pi{}t}{T})$ during the up-pulse, where $A$ is the pulse amplitude (set to one-fourth of the estimated bottleneck bandwidth) and $T$ is the pulse duration, and measures the cross traffic's response in the frequency domain.
% The \inbox can use the Nimbus algorithm to detect when to relinquish control over the queue by interposing this sending pattern over the delay-controller's rate decisions.
% However, if the \inbox entirely drains its queues into the network, it will no longer be possible for Nimbus to overlay pulses onto the traffic pattern, and it will be unable to determine the nature of the cross traffic.
% Practically, this would mean that once \inbox switches to compete with cross traffic, it would never gain the information necessary to switch back.

To support the Nimbus pulses while also letting the traffic pass,
the \inbox must maintain sufficient queueing for the up-pulse,
\ie the area under the up-pulse curve: 
$A \int_0^{\frac{T}{4}} \sin(\frac{4\pi{}t}{T}) dt = \frac{AT}{2\pi}$.
From Nimbus, we use $T = 0.2$ seconds and $A =$ one-fourth the bottleneck bandwidth. Then, by Little's law, the amount of extra queueing necessary is equivalent to
$\frac{T}{8\pi}$, or $8$ms. % We call this the target queueing delay, or $q_T$. 
We thus configure the \inbox to maintain a target queueing delay of $q_T = 10$ms (the additional $2$ms is a cushion against input variance).
Because bundled connections will experience this queueing in addition to other queueing in the network, 
most traditional congestion control algorithms (\eg Cubic) will observe RTT inflation. 
In \S\ref{s:robust:cross} we show that this effect is not large; \name still achieves performance comparable to the status quo. Nevertheless, it is desirable to be as close to $q_T$ as possible. 

Thus, to achieve the target queueing delay $q_T$
we use a PI controller at the \inbox which determines how the base sending rate $r(t)$ should be updated:
$\dot{r}(t) = \alpha (q(t) - q_T) + \beta (\dot{q}(t))$, where $\dot{r}(t)$ is the update to $r(t)$ before imposing the Nimbus pulse, $q(t)$ is the current queue size at the \inbox in milliseconds, and $\alpha$ and $\beta$ are both positive.
If $q(t) > q_T$, the first term will be positive and the rate will increase, which will decrease the queueing delay. Similarly, if the queue size is growing, the second term will be positive and the queueing delay will decrease. 
Setting $\alpha$ and $\beta$ controls a tradeoff: with larger values the controller will approach the target queueing delay faster, but if they are too large the controller's variations will dominate the Nimbus pulse. 
If they are too small, it will take too long to reach the target delay. 
We found that $\alpha=10$ and $\beta = 10$ work well for the scenarios in our evaluation (\S\ref{s:eval} and \S\ref{s:eval:realworld}).

% This problem is similar to the role of the PIE AQM mechanism~\cite{pie}, which also seeks to maintain a queueing delay target.
% Correspondingly, we design a PI controller at the \inbox as part of the fairness control module. 
% It overlays a rate $r$ corresponding to , where $q$ is the queue size and $q_T$ is the target queue size computed above.
% We pick $\alpha = 10$ and $\beta = 10$ by solving for a convergence time of one RTT (Appendix~\ref{app:derive-ab}).
% as per \S\ref{s:qctl:pi}.

\subsection{Imbalanced Multipathing}\label{s:queue-ctl:ecmp}
Since a \bundle contains many component connections, a load balancer may send them along different paths. If the load along different paths is well-balanced, \name will accurately treat a load-balanced bottleneck link as a single link whose rate is the sum of the rates of each sub-link. However, when the load along different paths is imbalanced, the series of measurements \name collects will be a random sampling of the different paths, which would confuse the delay-control algorithm and cause it to perform poorly. Fortunately, such cases are straight-forward to detect with our measurement technique. 
More specifically, load imbalance will result in many epoch measure packets arriving out-of-order at the \outbox (whenever epoch packet $i$ happens to traverse a path with a larger delay than epoch packet $i+1$), and consequently, out-of-order ``congestion ACKs'' at the \inbox.  Figure~\ref{fig:queue-ctl:ecmp:motivation} demonstrates this in an emulated imbalance scenario. 

Therefore, we use the fraction of epoch measurement packets that arrive out-of-order as an indicator of load imbalance due to multipathing.
If this number is small, the links are roughly balanced and \name will operate as expected.
If it is large, it indicates load imbalance, in which case \name's rate control may not work well. 
%While we believe that it is possible to design a rate controller that distinguishes RTT samples from different paths, and computes appropriate aggregate rates, we leave this for future research, and instead adopt a simpler
%the delay controller will make erratic adjustments as it briefly observes high RTTs, and may lose throughput as a result.
In \S\ref{s:eval:ecmp}, we experimentally determine an out-of-order fraction of 5\% to be a good threshold indicating whether or not the links are balanced: all single-path scenarios resulted in an order of magnitude fewer out-of-order packets, and all multi-path scenarios resulted in an order of magnitude greater.

% Therefore, if the reordering level is above 5\%, we disable rate control and revert to status quo performance. We evaluate this approach (and justify our chosen threshold) in \S\ref{s:eval:ecmp}.
%In this case, the measurement sub-system (\S\ref{s:measurement}) would report erratic measurements corresponding in turn to each of the possible paths.
% To see why, consider that \name's epoch-based rate measurements do not distinguish the path the packets take, and instead consider only the number of packets sent and received in an epoch.
% Thus, \name will accurately measure a load-balanced bottleneck link as a single link with the aggregate rate of each sub-link.
% However, for RTT measurements, the link with the lowest RTT will be over-sampled because the first epoch feedback packet to arrive will be from this link.
% So, \name will not detect building queues in other links.
% In most cases, this is not fatal: the delay controller might yield too much queue to the bottleneck, causing \name to miss out on performance improvements.
% We leave the design of a delay controller that does not over-react to conflicting measurements, and thus further optimizes traffic-control ability, to future work.

%In extreme cases, there may be persistent imbalance in the level of queueing. 
\input{ecmp_delay}

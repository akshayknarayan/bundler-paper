\section{Overview}
\label{s:deploy}

Figure~\ref{fig:deploy:arch} provides an example scenario for deploying \name. 
\name aggregates traffic between Domain A and Domain B into the same bundle. 
Then, on egress, the \inbox moves the in-network queues built by the bundled traffic to itself (we describe these mechanisms in \S\ref{s:design}). 
It can thus enforce desired scheduling policies across the traffic in the bundle.

The performance benefits that this provides are dictated by the following.

\Para{Amount of Aggregation} 
Bundles are naturally \emph{composable}: a subdomain of domain A could deploy its own \name to take control of its fraction of the in-network queues.
%However, the amount of aggregated traffic in a bundle correlates with the scheduling opportunities within it, thus influencing the performance benefits that \name provides. 
However, a traffic bundle, to be useful, must be \emph{heavyweight} enough to drive queueing in the network; these queues, once \name controls them, provide scheduling opportunity.
A natural question, then, is: would a given bundle have sufficient traffic in practice to make a \name deployment beneficial? 
Past observations indicate a positive response to this question, since a majority of Internet traffic today is owned by a few large content providers who host a wide array of services~\cite{fivecomps, labovitz}. 
In our example scenario, the bundle from Domain A to B might be comprised of large amounts of traffic generated by various services (such as email, messaging, video conferencing, video streaming, cloud storage etc.) hosted by the content provider and used by different clients within the enterprise.

\an{move to eval?}
An interesting scenario is load-balancing in the core, a widely used technique for reducing the congestion on any one path.
Here, we make two observations: 
(1) while bundle traffic may traverse multiple intermediate paths, it necessarily must converge on the receiving domain (and the \outbox). If the bottleneck is near the receiving domain, the bundled traffic will still build queues.
(2) from the bundle's perspective, multiple paths appear as multiple queues, with some aggregate bandwidth capacity (\ie the max-flow of the network paths between the sending and receiving domains). While queueing may be unevenly distributed among the paths, the distribution of flows onto paths is currently random (\eg with ECMP). With \name, all these queues are moved to the \inbox, so it can schedule bundled flows deliberately rather than randomly.
In \S\ref{s:eval}, we show that \name still controls queueing on bundles in the presence of multipath.

\Para{Congestion in the middle of the network} 
Customers already have control over queues that are built within their own domains~\cite{swan, b4, bwe}. \name, therefore, provides benefits when congestion occurs, and queues build up, in the middle of the network (\ie between a \pair).  
So the next question that arises is, does such in-network congestion occur in practice? A recent measurement study~\cite{inferring-interdomain-congestion} indicates that inter-domain links in the network (such as the red bottleneck link in Figure~\ref{fig:deploy:arch}) can indeed experience significant congestion. 
We briefly discuss how the nature of this congestion influences \name's benefits (and provide detailed results in \S\ref{s:eval}).

\paragraphi{Self-inflicted congestion} This occurs when traffic from a single bundle causes a queue to build up at the bottleneck links in the network, even without any other cross-traffic. It can happen when a small carrier network does not have enough capacity to sustain the large volume of traffic sent by a content provider to a receiving domain, or due to explicit rate limiting commonly done by ISPs~\cite{isp-throttle-1, isp-throttle-2, isp-throttle-3}. In such cases, \name can result in
significant improvements in performance, as it would then have control over the entire queue. In our experiments (\S\ref{s:eval}), we found this to be the case.

\paragraphi{Congestion due to bundled cross-traffic} In-network congestion can further increase in the presence of other cross-traffic (\eg when the peering link between Carrier B and the enterprise in Figure~\ref{fig:deploy:arch} is shared by traffic from multiple sending domains). \name continues to provide benefits when such competing flows also belong to bundles created by the {\inbox}es deployed in other domains; the rate control algorithm at each of these {\inbox}es would ensure that the in-network queues remain small, and appropriate scheduling policy would be applied to the per-bundle queues built at the {\inbox}es.
We expect this to become the common form of observed congestion as more domains deploy \name. 

\paragraphi{Congestion due to un-bundled cross-traffic} We now consider the scenario where the cross-traffic includes flows from domains that have not yet deployed a \name. If all such \emph{un-bundled} competing flows are short-lived (up to a few MBs), the bundled traffic still sees significant performance benefits. 
However, if the cross traffic includes a persistent backlogged flow that aggressively fills up any available buffer space at the bottleneck link, then, in order to compete fairly, \name would push more packets into the network (as described in \S\ref{s:impl:prototype}), thus relinquishing its control over the bundled traffic and falling back to the status quo performance. 
This scenario is a limitation of \name's design, since it forces \name to fall back to status-quo performance.
\an{reword} Such flows are unlikely to arrive very frequently~\cite{caida-dataset}. 

\vspace{0.05in}
\paragrapha{Takeaway} While \name must revert to status quo performance in the face of aggressive, buffer-filling cross traffic, in most scenarios it can significantly improve performance.
This, combined with its deployment ease, makes a strong case for deploying \name. 

\section{Example Scenarios}\label{s:deploy}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{img/shift-bottleneck-combined}
    \caption{This illustrative example with a single flow shows how \name can take control of queues in the network. The plots, from measurements on and emulated path (as in \S\ref{s:eval}), show the trend in queueing delays at each queue over time. The queue where delays build up is best for scheduling decisions, since it has the most choice between packets to send next. Therefore, the \inbox \emph{shifts} the queues to itself to gain scheduling power.}\label{fig:design:shift-bottleneck}
\end{figure*}
%

%\fc{Would it be better to lead with some example of how theres a large amount of traffic bewteen A and B? }

Figure~\ref{fig:deploy:arch} provides an example scenario for deploying \name. 
\name aggregates traffic from Domain A to Domain B, and vice-versa, into two unidirectional bundles. 
Then, on egress, the \inbox moves the in-network queues built by the bundled traffic to itself (we describe the specific mechanism in \S\ref{s:design}). 
It can thus enforce desired scheduling policies across the traffic in the bundle.
The performance benefits that this provides are dictated by the following.

\Para{Amount of Aggregation} 
A traffic bundle, to be useful, must be \emph{heavyweight} enough to drive self-inflicted queueing in the network; these queues, once under \name's control, provide scheduling opportunities. We expect many bundles to be heavyweight in practice because a majority of Internet traffic today is owned by a few large content providers who host a wide array of services~\cite{fivecomps, labovitz}. 
In our example scenario, the bundles between Domain A and B might comprise large amounts of traffic generated by various services (such as email, messaging, video streaming, cloud storage etc.) hosted by the content provider and used by different clients within the enterprise. 
%\radhika{The bundle from domain B to A would similarly be heavyweight, though possibly lighter than the bundle in the opposite direction.}
%\an{maybe good to list examples of what this would consist of, in case people just think of ACKs. Maybe backups? But several of the applications above (\eg video) are already bi-directional.}
%Furthermore, it is possible to observe self-inflicted queueing even by sending from a single machine; we take advantage of this in experiments on real Internet paths in \S\ref{s:eval}. \radhika{chop last line?}

\Para{Congestion in the middle of the network} 
Content providers already have control over packets which queue within their own domains~\cite{swan, b4, bwe}. \name is useful for moving queues that build up due to congestion in the middle of the network (\ie between a \pair).
The next question that arises is, does such in-network congestion occur in practice? A recent measurement study~\cite{inferring-interdomain-congestion} indicates that inter-domain links in the network (such as the red bottleneck link in Figure~\ref{fig:deploy:arch}) can indeed experience significant congestion. 
We briefly discuss how the nature of this congestion influences \name's benefits (and provide detailed results in \S\ref{s:eval}).

\paragraphi{Self-inflicted congestion} This occurs when traffic from a single bundle causes a queue to build up at the bottleneck links in the network, even without any other cross-traffic. It can happen when a small carrier network does not have enough capacity to sustain the large volume of traffic sent by a content provider to a receiving domain, or due to explicit rate limiting commonly done by ISPs~\cite{isp-throttle-1, isp-throttle-2, isp-throttle-3}. In such cases, \name can result in
significant improvements in performance, as it would then have control over the entire queue. 
%In our experiments (\S\ref{s:eval}), we found this to be the case.

\paragraphi{Congestion due to bundled cross-traffic} In-network congestion can further increase in the presence of other cross-traffic (\eg when the peering link between Carrier 2 and the enterprise in Figure~\ref{fig:deploy:arch} is shared by traffic from multiple content providers' domains). 
\name continues to provide benefits when the competing flows are part of other bundles from/to other domains because the rate control algorithm at each of the other {\inbox}es would ensure that the in-network queues remain small. Since each \inbox controls its own delays, it can apply the appropriate scheduling policy in its the per-bundle queues.
We expect this to become the common form of observed congestion as more domains deploy \name. 

\paragraphi{Congestion due to un-bundled cross-traffic} We now consider the scenario where the cross-traffic includes flows from domains that have not yet deployed a \name. If all such \emph{un-bundled} competing flows are short-lived (up to a few MBs), the bundled traffic still sees significant performance benefits. 
However, if the cross traffic is long lasting and aggressively fills up available buffer space at the bottleneck link, using a delay-based congestion controller  at the \name would adversely impact the bundled traffic. 
Instead, \name detects the presence of such \emph{buffer-filling} cross-traffic and, to compete fairly, pushes more packets into the network. 
It thus relinquishes most of its control (and scheduling opportunities) over the bundled traffic, while still maintaining a small queue for continued detection of cross-traffic (as detailed in \S\ref{s:queue-ctl}). This results in a slight throughput degradation with \name that we evaluate in \S\ref{s:robust:cross}.
%However, in these cases, \name chooses to create extra queueing as described in \S\ref{s:queue-ctl} to be able to detect when the cross traffic leaves.
%Therefore, the bundle's aggregate throughput will decrease slightly due to RTT unfairness.
%We evaluate this phenomenon in \S\ref{s:robust:cross}.
However, buffer-filling cross traffic is \emph{rare}.
Recent study in CDNs~\cite{akamai-cdn-trace} and packet trace from an Internet backbone router~\cite{caida-dataset} reveal that the vast majority of requests are smaller than 1MB; these requests may fill buffers for a few seconds, but no more. Our experiments on real Internet paths (\S\ref{s:eval:realworld}), conducted over multiple days also did not encounter such pathological cross traffic. 
\radhika{Akshay, check this out.}
%We further analyze a packet trace from an Internet backbone router~\cite{caida-dataset}.
%The flow size distribution here is even more skewed: $97.6\%$ of flows are under $10$KB, and only $0.02\%$ of flows are larger than $5$MB.

\Para{Shared congestion across flows in the bundle} Bundlerâ€™s design for moving queues via aggregate congestion control assumes that the component flows within a bundle share in-network congestion. We believe this would hold in most scenarios. Even if an ISP does load balancing for its core, the bottleneck interdomain links would still be shared by the flows in a bundle. Another common scenario is for the bottleneck to be at the edge of the receiving domain, which will again be shared across the bundled flows. Finally, even if an in-network load balancing mechanism splits the bundled flows across multiple congested paths with multiple in-network queues, \name would account for the aggregate network bandwidth and move all of these queues to the sender's domain. Our success with using \name on real Internet paths (\S\ref{s:eval:realworld}) suggests that our design is robust to such multi-pathing effects. 

%(2) from the bundle's perspective, multiple paths appear as multiple queues, with some aggregate bandwidth capacity (\ie the max-flow of the network paths between the sending and receiving domains). While queueing may be unevenly distributed among the paths, the distribution of flows onto paths is currently random (\eg via ECMP). With \name, all these queues are moved to the \inbox, so it can schedule bundled flows deliberately rather than randomly.

\vspace{0.05in}
\paragrapha{Takeaway} While \name must yield its scheduling ability in the face of aggressive, buffer-filling cross traffic, in most scenarios it significantly improves performance (as we show in \S\ref{s:eval}).
This, combined with its deployment ease, makes a strong case for deploying \name. 

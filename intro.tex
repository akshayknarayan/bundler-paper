\section{Introduction}\label{s:intro}


%\radhika{Alternate intro: mostly just changed the flow of certain points, removed some that might be better suited in related work, and made some more concrete/specific. /*}

%lots of scheduling algorithms proposed. scheduling used in private wans. seems clearly beneficial.
The vast literature on scheduling and queue management algorithms~\cite{diffserv, fair-queueing, sfq, pie, CoDel, fifoplus, virtualClocks, csfq, drr, red, ecn} clearly indicates the performance benefits of deploying such schemes within the network. However, the public Internet has, at large, been unable to enjoy these benefits. 

Although router vendors have, over the years, implemented certain schemes (such as strict priorities, round-robin for fair queueing, and dynamic buffer limiting for active queue management~\cite{cisco-qos}), they are often not enabled, or used effectively, by the network operators. 
%\radhika{edit, more than not being enabled, they cannot be enforced effectively}
%Meanwhile, in privately operated wide-area networks, operators have used simple scheduling policies in the core to reap significant gains~\cite{b4, bwe, swan}. 
This is primarily due to the challenging position of operators in the public Internet, where each of their customers may desire diverging, and often incompatible, features. For example, while enabling fair queueing in the routers would benefit short interactive flows competing with bulk transfers, it may penalize flows bundled within a single VPN tunnel by allocating them smaller than their fair share of bandwidth. 
%Similarly, while enabling an AQM scheme might reduce the queuing delay in the absence of a BBR flow, it may severely penalize non-BBR flows that compete with BBR. (include if we find a citation). 
%\radhika{something about not knowing how to apply policies across customers}
%\radhika{similar issue arises with our bundler too...}.
Thus, due to limited visibility into their customers' traffic, network operators cannot choose policies that can appease them all. 

An individual customer can, on the other hand, choose appropriate scheduling and AQM policies for the \emph{component flows} within its own traffic as per its own requirements. 
%; that is, scheduling policy is easier to express at the edge. 
Examples include traffic over the public Internet between different datacenters, between campuses of an organization, collaborating universities or companies, between large content provider (\eg Netflix) and a network with many clients (e.g., a regional ISP), large-scale data backups from an organization to an external site (\eg Dropbox), and so on. 
In each of these scenarios the sender has enough information to unilaterally choose a scheduling or queue management policy for its component flows (such as prioritizing flows based on their application types, enforcing fair queuing for a more stable rate allocation, using AQM to achieve a delay target, and so on). However, these policies are effective only when enforced on congested bottleneck links in a flow's path that experience a queue build-up, and such links are often outside the control of the individual customers~\cite{inferring-interdomain-congestion}. 

So while network operators do not have enough visibility into the traffic to choose the appropriate policy, the customers may not have enough control over the queued-up links in the public network to enforce their desired policies. This paper attempts to resolve this conundrum by simply \emph{moving the queues in the public network to the customer's edge}, such that appropriate scheduling and queue management policies can be effectively enforced. The natural question that arises is, how can the queues be moved? This is the primary focus of our work. 

We begin with the observation that component flows of the traffic from a given sender's domain and destined for the same downstream domain, as in the examples mentioned earlier, are likely to share common bottlenecks within the network. By (i) aggregating such component flows into a single \emph{bundle} at the sender's egress, and (ii) controlling the outgoing rate of this bundle at the egress to match its bottleneck rate in the network, we can effectively \emph{move} the queues built by the traffic in the bundle, from within the network to the sender's egress itself. \radhika{is the last sentence too long / hard to understand? suggestions for fixing it?} 

We further note that middleboxes are now prevalent in the Internet architecture~\cite{aplomb}. They are often deployed at the egress and ingress of an enterprise network or a domain, to ensure traffic transits them for intrusion detection, packet inspection, filtering etc. They thus provide an effective vantage point for aggregating and managing traffic. 

We, therefore, propose a new type of middlebox, called a \name, that (1) aggregates the traffic at the customer's egress into appropriate bundles whose component flows share a common receiver domain, (2) does rate control for each bundle to shift the corresponding queues from within the network to itself, and (3) applies scheduling and queue management policies within each bundle as per the customer's requirements. 

%\radhika{add a few lines on \name's high level architecture.} 
 A \name comprises of a pair of middleboxes: a \inbox that sits at the sender's egress and a \outbox that sits at the receiver's ingress. Thus, a bundle is defined as a group of flows that traverse through the same \inbox and \outbox pair. The bulk of \name's functionality resides at the \inbox, which coordinates with the \outbox for (i) classifying the flows into appropriate bundles, and,  (ii) collecting congestion signals, such as the round-trip time, and the rate at which bundled traffic arrives at the \outbox. These signals are used by a standard congestion control algorithm (with certain properties, as described later) that runs at the \inbox and computes appropriate sending rates for each bundle. The computed sending rates are such that the queuing induced by the bundled traffic in the middle of the network is minimized and is incurred at the \inbox instead, while ensuring that the bottleneck links in the network are not under-utilized. 
 
 We introduce a novel light-weight methodology for the \inbox and \outbox coordination, which does not require any per-flow state (only requiring per-bundle state), and does not require any modifications to the packets traversing the \name boxes. Furthermore, our overall approach only requires introducing the \name boxes at the customer's edges, and requires no changes to the individual end-hosts or to the routers in the public Internet. 
 
 Notice that our goal is to only do scheduling and queue management across the traffic within the same bundle (and therefore belonging to a single customer). We do not even attempt to tackle the more ambitious objective of  scheduling traffic across different customers, which would require dealing with the potential differences in their requirements. Furthermore, as we will discuss later, there can be instances where \name fails to provide any performance benefits for the bundled traffic, and falls back to the status quo performance. Nonetheless, we believe that our work, in its simplicity, provides an \emph{immediately deployable} solution for enabling some of the benefits of scheduling and queue management in the public Internet, which is something that has been missing from the scheduling literature so far. 
 %\radhika{should i move it up by 2 paragraphs (i.e. right after the 3 points)?} 

% We make the following key contributions:
% \begin{enumerate}
%     \item The design and implementation of a \name, including a novel method of collecting congestion control information and enforcing the decisions of a rate control algorithm on traffic aggregates, which \emph{moves} the queues from the bottleneck in the network to the customer's edge.
%     \item An evaluation of the benefits of scheduling and queue management for traffic aggregates, compared to both the status quo (FIFO) and an idealized deployment where bottleneck queues deploy the desired policy.
% \end{enumerate}

%The rest of the paper dives into the details of how we design, implement and evaluate \name, and is organized as follows:
We begin the rest of this paper by discussing some related work in \S\ref{s:related}. We then discuss our design choices, along with the favorable operation regime for \name in \S\ref{s:design}. We detail our novel \inbox and \outbox coordination methodology for collecting congestion signals in \S\ref{s:measurement}, and describe our prototype implementation in \S\ref{s:impl}. 
%\S\ref{s:design} describes the \name architecture and the various trade-offs we considered when designing it.  
%\S\ref{s:measurement} describes our novel approach for gathering measurements from the network to do rate control at the \name in order to move the bottleneck. 
%\S\ref{s:impl} describes our prototype implementation. 
Finally, in \S\ref{s:eval}, we use our prototype implementation to evaluate the performance benefits of using a \name across a wide variety of emulated scenarios, before concluding in \S\ref{s:concl}.
\radhika{revise}
%Middleboxes offer a flexible design point, since implementations spanning hardware and software represent numerous options in implementing scalable, yet flexible and sophisticated scheduling policies. 

%Consequently, the emergence of programmable scheduling~\cite{pifo} might also not be a panacea; while it might make it simpler to implement different scheduling policies, their enforcement would remain difficult due to lack of sufficient knowledge about the customers' requirements. 

%On the other hand, even simple policies, such as priority scheduling across a small number of traffic classes, deployed within privately operated wide-area networks, have been shown to provide significant gains in performance~\cite{b4, swan, bwe} \radhika{can we also show some results with priority scheduling in eval?}.  
%, and the deployment of simple scheduling policies in privately ~\cite{b4, bwe, swan} clearly indicates the benefits of 

%individual customers can choose, but can't enforce scheduling at edge. makes most sense to do it an congested routers, out of their control.

\cut{
\radhika{*/ Old intro:}


Most Internet paths today employ first-in, first-out (\emph{FIFO}) scheduling \radhika{I think they might also do some coarse-grained priorities. Let's not make this our first sentence, unless we are absolutely sure.}. 
However, traffic could benefit from different scheduling algorithms.
The bottleneck link, where congestion occurs and therefore where scheduling is the most useful, is often at the edge of a network, whether an individual device or an autonomous system~\cite{inferring-interdomain-congestion} \radhika{the last part of this sentence not super clear to me.}. 
As a result, deploying scheduling algorithms on one's own routers is of limited utility; often, the congestion occurs in a router outside of one's control.
Unfortunately, networks have proven difficult to evolve; although router vendors have, over the years, implemented other scheduling algorithms (such as fair queueing~\cite{fair-queueing}), operators often do not enable these features.
Thus, the emergence of programmable switching~\cite{p4} is not a panacea; while it might now be simpler to implement scheduling policy, it remains difficult to see deployment \radhika{P4 does not ease deployment of scheduling algorithms.}.
Meanwhile, in privately operated wide-area networks, operators have used scheduling in the core to reap significant gains~\cite{b4, bwe, swan}. 
Clearly, the use of scheduling policy is beneficial.

Why, then, are operators hesitant to deploy new scheduling policies on congested routers? 
First, scheduling policy can be difficult to configure, and poor configurations can cause significant performance degradation~\cite{nanog-discussion}.
Second, the position of operators in the public Internet is a challenging one, as each of their various customers may desire diverging (often incompatible) features. For example, due to the prevalence of VPN tunnelling, enabling fair queueing in routers may penalize flows bundled within a VPN tunnel.
Thus, operators cannot see the full picture, and cannot offer scheduling that will appease any one customer.

Crucially, however, \emph{individual customers} can often choose a policy that suits their needs; that is, scheduling policy is easier to express at the edge. 
Examples include: traffic between different datacenters over the public Internet; traffic between campuses of an organization; traffic between collaborating universities or companies; traffic between a large content provider (\eg Netflix) and a network with many clients (e.g., a regional ISP); large-scale data backups from an organization to an external site (\eg Dropbox); etc. 
In each of these scenarios the sender has enough information to unilaterally choose a scheduling policy for its component connections, but usually lacks the control over the bottleneck link necessary to realize it.

While it may be easier to \emph{express} scheduling policy near the edge, it is not immediately obvious how to \emph{implement} it \radhika{express vs implement distinction might be too subtle for first time readers}. 
Traffic from a domain is naturally comprised of numerous individual connections;
each connection independently probes for bandwidth, detects congestion on its path, and reacts accordingly \radhika{this sentence seems a bit out-of-blue. a connector to previous sentence needed.}.

\radhika{need a brief overview of bundler architecture (maybe the middlebox paragraph that appears later), before diving into technical challenges below.}

We address this challenge in two steps. 
First, we treat the traffic from multiple component connections as a \emph{single aggregate}, \ie as a router would view them \radhika{what does the last part of the sentence mean?}. We decide a combined sending rate for the aggregate. Then, inside the aggregate, we enforce a scheduling policy among the component flows. 
In other words, we decouple the aggregate rate decision from the scheduling policy.
This approach, which we call {\em aggregate traffic control}, therefore has two technical challenges:
\begin{enumerate}
    \item {\bf Congestion control.} At what combined rate should the traffic aggregate send packets? This depends on network conditions at the bottleneck, which the traffic aggregate cannot directly observe.
    Fortunately, this is exactly the question answered by congestion control algorithms; they take as input measurements of network conditions -- RTT, sending rate, receiving rate, etc -- and yield as output a rate at which to send packets.
    Furthermore, since the traffic aggregate as a whole determines the sending rate, this approach causes component flows share congestion information.
    Thus new flows can discover and adapt to their path's congestion conditions much more quickly, which is particularly beneficial for short flows~\cite{CM}. 
    
    \item {\bf Flow scheduling.} Given a combined rate, in what order should the aggregate transmit packets?  This enables fine-grained scheduling for flows within a traffic aggregate. 
    For example, we can schedule flows based on size (short flows first), application priorities, deadlines, etc.
\end{enumerate}

How should we implement and deploy our approach? 
We note that middleboxes are now prevalent in the Internet architecture~\cite{aplomb}.
The rise of middleboxes allows a new vantage point into flows, since middleboxes are often deployed at natural ``choke points'' \radhika{choke point by definition means a bottleneck -- any other word we can use instead?} in a network to ensure traffic transits them for \eg traffic analysis or intrusion detection.
Middleboxes offer a flexible design point, since implementations spanning hardware and software represent numerous options in implementing scalable, yet flexible and sophisticated scheduling policies.
We therefore propose a new type of middlebox to perform traffic aggregation, which we call a \name.

We make the following contributions:
\begin{enumerate}
    \item The design and implementation of a \name, including a novel method of collecting congestion control information and enforcing the decisions of a congestion control algorithm on traffic aggregates. This method \emph{moves} the queues in the network from the bottleneck to the edge.
    \item An evaluation of the benefits of scheduling for traffic aggregates, compared to both the status quo (FIFO) and an idealized deployment where bottleneck queues deploy a scheduling algorithm.
\end{enumerate}

The rest of this paper is organized as follows: 
\S\ref{s:design} describes tradeoffs in the design of a \name. 
\S\ref{s:measurement} describes our approach to gathering measurements from the network. 
\S\ref{s:impl} describes our prototype implementation. 
\S\ref{s:eval} shows an evaluation of the benefits of \name.

}
\cut{
\begin{outline}
\1 Previously, this approach has required terminating the TCP connection at the middlebox; \ie implementing a TCP proxy.
    \2 TCP proxies are widely used to add congestion control functionality in the network.
    \2 \an{drawbacks of TCP proxies}
        \3 head of line blocking
        \3 implementation complexity
        \3 end-to-end principle
\end{outline}
}


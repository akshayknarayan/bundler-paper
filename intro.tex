\section{Introduction}\label{s:intro}


%\radhika{Alternate intro: mostly just changed the flow of certain points, removed some that might be better suited in related work, and made some more concrete/specific. /*}

%lots of scheduling algorithms proposed. scheduling used in private wans. seems clearly beneficial.
The vast literature on scheduling and queue management algorithms~\cite{diffserv, fair-queueing, sfq, pie, CoDel, fifoplus, virtualClocks, csfq, drr, red, ecn} clearly indicates the performance benefits of deploying such schemes within the network. However, the public Internet has proven difficult to evolve accordingly; although router vendors have, over the years, implemented certain schemes (such as fair queueing~\cite{fair-queueing}\radhika{might be better to cite deficit round robin. need to double check}), they are often not enabled by the network operators. 
%Meanwhile, in privately operated wide-area networks, operators have used simple scheduling policies in the core to reap significant gains~\cite{b4, bwe, swan}. 
This reluctance has stemmed from valid concerns. First, such policies can be difficult to configure, and poor configurations can cause significant performance degradation~\cite{nanog-discussion}. Second, the position of operators in the public Internet is a challenging one, as each of their customers may desire diverging (and often incompatible) features. For example, while enabling fair queueing in the routers would benefit short interactive flows competing with bulk transfers, it may penalize flows bundled within a single VPN tunnel by allocating them smaller than their fair share of bandwidth.
%\radhika{similar issue arises with our bundler too...}.
Thus, due to limited visibility into their customers' traffic, network operators cannot choose policies that can appease them all. 

On the other hand, an individual customer can effectively choose a scheduling policy for the \emph{component flows} within its own traffic as per its own requirements. 
%; that is, scheduling policy is easier to express at the edge. 
Examples include traffic over the public Internet between different datacenters, between campuses of an organization, collaborating universities or companies, between large content provider (\eg Netflix) and a network with many clients (e.g., a regional ISP), large-scale data backups from an organization to an external site (\eg Dropbox), and so on. 
In each of these scenarios the sender has enough information to unilaterally choose a scheduling policy for its component flows. However, these policies are effective only when enforced on congested bottleneck links in a flow's path that experience a queue build-up, and such links are often outside the control of the individual customers~\cite{inferring-interdomain-congestion}. 

So while network operators do not have enough visibility into the traffic to choose the appropriate policy, the customers may not have enough control over the queued-up links to enforce their desired policies. This paper attempts to resolve this conundrum by simply \emph{moving the queues in the network to the customer's edge}, such that appropriate scheduling and queue management policies can be effectively enforced.

The natural question that arises is, how can the queues be moved? We observe that the component flows of the traffic from a given sender's domain and destined for the same downstream domain, as in the examples mentioned earlier, would share common bottlenecks within the network. Aggregating these component flows at the sender's egress, and controlling the outgoing rate of this aggregate to match its share at the bottleneck link, would \emph{move} the corresponding queues from the bottleneck in the network to the sender's egress itself \radhika{need to simplify this sentence}. 

We further note that middleboxes are now prevalent in the Internet architecture~\cite{aplomb}. They are often deployed at the egress and ingress of an enterprise network or a domain, to ensure traffic transits them for intrusion detection, packet inspection, filtering etc. They therefore provide an effective vantage point for aggregating and managing traffic. 

We therefore propose a new type of middlebox, called a \name, that (i) performs traffic aggregation and rate control at the egress of a customer's edge to shift the queues for the traffic to itself, and (ii) enforces scheduling and queue management policies as per the customer's requirements (such as adhering to a desired delay target, prioritizing flows based on their size, application type, or deadline, enforcing fair queuing for a more stable rate allocation and so on). 

\radhika{add a few lines on \name's high level architecture.}

% We make the following key contributions:
% \begin{enumerate}
%     \item The design and implementation of a \name, including a novel method of collecting congestion control information and enforcing the decisions of a rate control algorithm on traffic aggregates, which \emph{moves} the queues from the bottleneck in the network to the customer's edge.
%     \item An evaluation of the benefits of scheduling and queue management for traffic aggregates, compared to both the status quo (FIFO) and an idealized deployment where bottleneck queues deploy the desired policy.
% \end{enumerate}

The rest of the paper dives into the details of how we design, implement and evaluate \name, and is organized as follows:
\S\ref{s:related} discusses related work.
\S\ref{s:design} describes the \name architecture and the various trade-offs we considered when designing it.  
\S\ref{s:measurement} describes our novel approach for gathering measurements from the network to do rate control at the \name in order to move the bottleneck. 
\S\ref{s:impl} describes our prototype implementation. 
\S\ref{s:eval} evaluates the performance benefits of using a \name for scheduling and queue management.
\radhika{re-write this to highlight strong points after the rest of the paper has been written}


%Middleboxes offer a flexible design point, since implementations spanning hardware and software represent numerous options in implementing scalable, yet flexible and sophisticated scheduling policies. 









%Consequently, the emergence of programmable scheduling~\cite{pifo} might also not be a panacea; while it might make it simpler to implement different scheduling policies, their enforcement would remain difficult due to lack of sufficient knowledge about the customers' requirements. 

%On the other hand, even simple policies, such as priority scheduling across a small number of traffic classes, deployed within privately operated wide-area networks, have been shown to provide significant gains in performance~\cite{b4, swan, bwe} \radhika{can we also show some results with priority scheduling in eval?}.  
%, and the deployment of simple scheduling policies in privately ~\cite{b4, bwe, swan} clearly indicates the benefits of 


%individual customers can choose, but can't enforce scheduling at edge. makes most sense to do it an congested routers, out of their control.




\cut{
\radhika{*/ Old intro:}


Most Internet paths today employ first-in, first-out (\emph{FIFO}) scheduling \radhika{I think they might also do some coarse-grained priorities. Let's not make this our first sentence, unless we are absolutely sure.}. 
However, traffic could benefit from different scheduling algorithms.
The bottleneck link, where congestion occurs and therefore where scheduling is the most useful, is often at the edge of a network, whether an individual device or an autonomous system~\cite{inferring-interdomain-congestion} \radhika{the last part of this sentence not super clear to me.}. 
As a result, deploying scheduling algorithms on one's own routers is of limited utility; often, the congestion occurs in a router outside of one's control.
Unfortunately, networks have proven difficult to evolve; although router vendors have, over the years, implemented other scheduling algorithms (such as fair queueing~\cite{fair-queueing}), operators often do not enable these features.
Thus, the emergence of programmable switching~\cite{p4} is not a panacea; while it might now be simpler to implement scheduling policy, it remains difficult to see deployment \radhika{P4 does not ease deployment of scheduling algorithms.}.
Meanwhile, in privately operated wide-area networks, operators have used scheduling in the core to reap significant gains~\cite{b4, bwe, swan}. 
Clearly, the use of scheduling policy is beneficial.

Why, then, are operators hesitant to deploy new scheduling policies on congested routers? 
First, scheduling policy can be difficult to configure, and poor configurations can cause significant performance degradation~\cite{nanog-discussion}.
Second, the position of operators in the public Internet is a challenging one, as each of their various customers may desire diverging (often incompatible) features. For example, due to the prevalence of VPN tunnelling, enabling fair queueing in routers may penalize flows bundled within a VPN tunnel.
Thus, operators cannot see the full picture, and cannot offer scheduling that will appease any one customer.

Crucially, however, \emph{individual customers} can often choose a policy that suits their needs; that is, scheduling policy is easier to express at the edge. 
Examples include: traffic between different datacenters over the public Internet; traffic between campuses of an organization; traffic between collaborating universities or companies; traffic between a large content provider (\eg Netflix) and a network with many clients (e.g., a regional ISP); large-scale data backups from an organization to an external site (\eg Dropbox); etc. 
In each of these scenarios the sender has enough information to unilaterally choose a scheduling policy for its component connections, but usually lacks the control over the bottleneck link necessary to realize it.

While it may be easier to \emph{express} scheduling policy near the edge, it is not immediately obvious how to \emph{implement} it \radhika{express vs implement distinction might be too subtle for first time readers}. 
Traffic from a domain is naturally comprised of numerous individual connections;
each connection independently probes for bandwidth, detects congestion on its path, and reacts accordingly \radhika{this sentence seems a bit out-of-blue. a connector to previous sentence needed.}.

\radhika{need a brief overview of bundler architecture (maybe the middlebox paragraph that appears later), before diving into technical challenges below.}

We address this challenge in two steps. 
First, we treat the traffic from multiple component connections as a \emph{single aggregate}, \ie as a router would view them \radhika{what does the last part of the sentence mean?}. We decide a combined sending rate for the aggregate. Then, inside the aggregate, we enforce a scheduling policy among the component flows. 
In other words, we decouple the aggregate rate decision from the scheduling policy.
This approach, which we call {\em aggregate traffic control}, therefore has two technical challenges:
\begin{enumerate}
    \item {\bf Congestion control.} At what combined rate should the traffic aggregate send packets? This depends on network conditions at the bottleneck, which the traffic aggregate cannot directly observe.
    Fortunately, this is exactly the question answered by congestion control algorithms; they take as input measurements of network conditions -- RTT, sending rate, receiving rate, etc -- and yield as output a rate at which to send packets.
    Furthermore, since the traffic aggregate as a whole determines the sending rate, this approach causes component flows share congestion information.
    Thus new flows can discover and adapt to their path's congestion conditions much more quickly, which is particularly beneficial for short flows~\cite{CM}. 
    
    \item {\bf Flow scheduling.} Given a combined rate, in what order should the aggregate transmit packets?  This enables fine-grained scheduling for flows within a traffic aggregate. 
    For example, we can schedule flows based on size (short flows first), application priorities, deadlines, etc.
\end{enumerate}

How should we implement and deploy our approach? 
We note that middleboxes are now prevalent in the Internet architecture~\cite{aplomb}.
The rise of middleboxes allows a new vantage point into flows, since middleboxes are often deployed at natural ``choke points'' \radhika{choke point by definition means a bottleneck -- any other word we can use instead?} in a network to ensure traffic transits them for \eg traffic analysis or intrusion detection.
Middleboxes offer a flexible design point, since implementations spanning hardware and software represent numerous options in implementing scalable, yet flexible and sophisticated scheduling policies.
We therefore propose a new type of middlebox to perform traffic aggregation, which we call a \name.

We make the following contributions:
\begin{enumerate}
    \item The design and implementation of a \name, including a novel method of collecting congestion control information and enforcing the decisions of a congestion control algorithm on traffic aggregates. This method \emph{moves} the queues in the network from the bottleneck to the edge.
    \item An evaluation of the benefits of scheduling for traffic aggregates, compared to both the status quo (FIFO) and an idealized deployment where bottleneck queues deploy a scheduling algorithm.
\end{enumerate}

The rest of this paper is organized as follows: 
\S\ref{s:design} describes tradeoffs in the design of a \name. 
\S\ref{s:measurement} describes our approach to gathering measurements from the network. 
\S\ref{s:impl} describes our prototype implementation. 
\S\ref{s:eval} shows an evaluation of the benefits of \name.

}
\cut{
\begin{outline}
\1 Previously, this approach has required terminating the TCP connection at the middlebox; \ie implementing a TCP proxy.
    \2 TCP proxies are widely used to add congestion control functionality in the network.
    \2 \an{drawbacks of TCP proxies}
        \3 head of line blocking
        \3 implementation complexity
        \3 end-to-end principle
\end{outline}
}

